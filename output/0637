&#xa0;
[normal]&#xa0;
[normal]Joint written evidence submitted by [bold]Adam J L Harris, Nigel Harvey, Leonard A Smith, [bold]David A Stainforth, Erica Thomp[bold]son [bold](CLC018)[bold]Executive Summary
[normal]Our main aim is to address the following point in the call for evidence from the Science and Technology Committee inquiry into “Climate: Public understanding and policy implications”:
[normal]We begin by discussing the wider context of the challenges and relevance of public engagement with science and with scientific uncertainties. We go on to comment on the way uncertainties in future climate are often characterised today. 
[normal]We end
[normal] with 
[normal]discussion of 
[normal]some significant issues regarding the character of climate prediction uncertainties and their relevance for public understanding.
[normal]To increase public trust in the results of climate science, w
[normal]e recommend that the uncertainties inherent in climate forecasts be made explicit. In cases where these uncertainties can be legitimately expressed as
[normal] probabilities, we suggest that use of a dual verbal/numerical format for expressing uncertainty (e.g., 
[normal]‘
[normal]likely (65-80%)
[normal]’
[normal]) will enhance communicative effectiveness. 
[normal] We also 
[normal]propose 
[normal]use of the verbal confidence scale developed by the 
[normal]Intergovernmental Panel on Climate Change for their 
[normal]Fifth Assess
[normal]ment Report
[normal], especially important i
[normal]n cases where there are underlying problems in expressing the uncertainties as probabilities
[normal]. Finally, we see the media as having a role in helping the public understand how forecasts are generated from models of the climate and, more particularly, 
[normal]the degree to which
[normal] diversity in outputs from these models reflects uncertainty in those forecasts.
[normal] 
[normal]Brief Introduction
[normal] to Submitters
[normal]This evidence has been collated by f
[normal]ive
[normal] people with relevant expertise. Adam Harris is a Lecturer, in the Department of Cognitive, Perceptual and Brain Sciences at UCL. He has published research on the communication of environmental risks within the context of climate change. Nigel Harvey is Professor of Judgment and Decision Research at UCL and Visiting Fellow in the Department of Statistics at LSE. 
[normal]He has published research on the role of judgment in forecasting
[normal], on determinants of trust,
[normal] and on the quality of subjective estimates of probability. Leonard Smith is Research Professor in the Department of Statistics at LSE and Director of the Centre for
[normal] the Analysis of
[normal] Time Series. He is also a Programme Director at the Centre for Climate Change Economics and Policy
[normal] and Senior Research Fellow at Pembroke College, Oxford
[normal]. He has published widely on the 
[normal]role of models in weather and climate research and on the interpretation of ensemble forecasts derived from such models. David Stainforth is Senior Research Fellow at the Grantham Research Institute on Climate Change and the Environment at LSE
[normal], where he is on the lead staff of the Global Response Strategies Research Programme
[normal]. He co-founded and was chief scientist on the climateprediction.net project, the world’s largest climate modelling experiment. 
[normal]His work focusses on how robust and useful information can be extracted from climate modelling experiments and on how to relate climate science to real-world decision making so that it will be of value to industry, policy makers and wider society.
[normal] E
[normal]rica Thompson is a Research Officer in the Centre for the Analysis of Time Series
[normal], working on the Munich Re Programme evaluating climate risks and opportunities.
[normal]&#xa0;
[normal]&#xa0;
[normal]Basis for recommendations
[normal]Introduction
[normal]1. 
[normal]Policy makers are frequently consider
[normal]ed
[normal] the primary consumers of statements about future climate. 
[normal] 
[normal]P
[normal]ublic support for policies
[normal], however
[normal], is
[normal] 
[normal]critical
[normal] 
[normal]to ensure their smooth implementation: they will be more effective if people comply willingly with them
[normal]. Such support
[normal] 
[normal]depends on people’s trust in both policy makers and their scientific advisors. T
[normal]hat t
[normal]rust
[normal], in turn, depends 
[normal]both 
[normal]on 
[normal]perceived competence and 
[normal]on 
[normal]perceived benevolence of motives: policy makers and their advisors may be mistrusted either because they are seen as well-meaning but incompetent or as competent but with a hidden agenda 
[normal]that favours vested interests.
[normal] 
[normal]2. Greater trust in scientific estimates of climate change is likely not just to increase support for government policies addressing it but also to lead to an increase in personal behaviours that may mitigate it. For example, people may reduce car use or increase home insulation.
[normal]3. 
[normal]Previous problems with policies based on uncertain science have arisen because people were given the impression that our knowledge was more certain than it was (BSE) or because, consequ
[normal]ent on this, they did not trust
[normal] 
[normal]assurances that our knowled
[normal]ge had a firm basis when it act
[normal]ually did 
[normal]so 
[normal](MMR). 
[normal]The lessons we need to learn from these events are that 1) we need to make levels of uncertainty associated with the science underpinning our policies 
[normal]explicit and 2) we need to make 
[normal]our rationale for those
[normal] uncertainty estimates expli
[normal]cit
[normal].
[normal] 
[normal]We need to do these things to increase trust. As Paul Slovic has pointed out, transparency (not withholding information) i
[normal]s a
[normal] fac
[normal]tor
[normal] that increases trust: it reassures people that there is no hidden agenda
[normal].
[normal]4
[normal]. C
[normal]limate modellers have developed a number of techniques that allow them 
[normal]to produc
[normal]e estimates of unce
[normal]r
[normal]tainties
[normal] associated with different degrees of climate change under specified assumptions. 
[normal] 
[normal]C
[normal]limate scientists 
[normal]do not
[normal],
[normal] 
[normal]however, always 
[normal]consider 
[normal]the diversity of their models to 
[normal]re
[normal]f
[normal]lect 
[normal]probabilities 
[normal]expressing 
[normal]uncertainties
[normal] associated with 
[normal]their 
[normal]forecasts.
[normal]  Moreover, a
[normal] model-based estimate of probability is always incomplete without an accompanying 
[normal](quantitative) 
[normal]estimate of our confidence that the model is able to provide meaningful information
[normal].
[normal] To put this the other way round, as is often the most usefu
[normal]l way of presenting it, what is
[normal] 
[normal]t
[normal]he probability that the model is misinformative
[normal]?
[normal]5
[normal]. It is also important to recognise
[normal] that different aspects of the climate problem have different character
[normal]istic
[normal]s of uncertainty. Basic science is sufficient to know that continued anthrop
[normal]o
[normal]genic emissions of greenhouse gases at the current rate will lead to 
[normal]a warmer planet and 
[normal]climate disruption with severe negative consequences for society. The details of how that will play out and the detailed national benefits of any particular miti
[normal]gation policy is still an area 
[normal]with substantial and conceptual uncertainties.
[normal]6
[normal]. 
[normal]In summary, 
[normal]people
[normal] must feel that current knowledge
[normal] is reasonably sound, 
[normal]that its limitations are being honestly communicated to them
[normal], and that policy makers take account of these limitations. This, in turn, implies that the public should somehow be made aware of the uncertainty estimates associated with climate forecasts
[normal] 
[normal](
[normal]including the uncertainty in so
[normal]-
[normal]called probabilistic projections
[normal]) 
[normal]and 
[normal]they should be helped to understand how we
[normal] can have confidence in the big picture without being able to predict the details. 
[normal]We now turn to problems associated with communicating 
[normal]this 
[normal]uncertainty.
[normal]7. Climate models have been used to produce “probability estimates” but 
[normal]these 
[normal]estimate
[normal]s
[normal] come from different 
[normal]methods with different 
[normal]assumption
[normal]s
[normal] and 
[normal]often 
[normal]different models
[normal]. We do not yet have methods to provide robust probabilities for many detailed aspects of 
[normal]future 
[normal]climate change.
[normal] Thus, there are two distinct challenges: one is to communicate probabilities when we have them; the other is to communicate uncertainty when such probabilities are not available. 
[normal]Communication of probabilistic 
[normal]information
[normal]8
[normal]. 
[normal]To address th
[normal]e first challenge
[normal], t
[normal]he Intergovernmental Panel on Climate Change (IPCC) adopted a policy of providing an explicit mapping between seven verbal terms and seven ranges of probability. 
[normal]The probabilities could then be expressed in verbal terms. 
[normal]Thus,
[normal] in the IPCC mapping
[normal], ‘virtually certain’ refer
[normal]s
[normal] to probabilities in the range 99 – 100% whereas ‘
[normal]likely
[normal]’
[normal] 
[normal]refer
[normal]s
[normal] 
[normal]to probabilities in the range 66
[normal] –
[normal] 100
[normal]%. 
[normal]This approach allowed results from experts who disagreed about precise numerical probabilities to be expressed in the same verbal terms. Thus, an expert who associated a probability of 70% with a particular future event and another expert who associated a probability of 90% with that event could both be said 
[normal]to assess the event as being ‘likely’.
[normal]9
[normal]. However, there are 
[normal]a number of 
[normal]problems 
[normal]associated 
[normal]with translating probabilities into verbal terms. 
[normal]First, t
[normal]erms, such as ‘virtually certain’ and ‘very likely’, are 
[normal]intuitively 
[normal]matched up to different ranges of probability by different people. Thus, ’very likely’ may refer to probabilities in the range 85 – 95% for one person but to 70 – 85% for another. 
[normal]Of particular relevance here
[normal], recent studies have shown that the particular mapping adopted by the IPCC is not consistent with how people naturally use the probability terms included in the IPCC mapping. 
[normal]Furthermore
[normal], the range of probabilities associated with particular verbal ex
[normal]pressions depends on various 
[normal]other factors, such as the seriousness and the expected frequency 
[normal](base rate) 
[normal]of the event to which the term refers
[normal]. Thus
[normal] 
[normal]e
[normal]xpressions referring to more severe outcomes are interpreted as denoting a higher probability
[normal].
[normal]10
[normal]. Given these problems with translating probability information into words, why should it not be communicated in its raw numerical form? There are two arguments against this. First, 
[normal]a single probability may give the impression that uncertainty estimates are more accurate than they are. This can be countered by using numerical ra
[normal]nges of probabilities (e.g., 70 – 
[normal]85%). Second, it is sometimes 
[normal]claimed that residents of the United Kingdom cannot easily understand probabilities and that information (e.g., weather forecasts) should therefore not be given probabilistically.
[normal] To address this, both verbal terms and a range of numerical probabilities could be used to express the uncertainty associated with a particular event. Thus, an event could be labelled as ‘likely (65 – 80%)’. Providing information in this dual form would supply probability information to those who can benefit from it without removing the verbal expressions of uncertainty from those who cannot.
[normal]11
[normal]. 
[normal]People need not be given probabilities (or ranges of probabilities) for partic
[normal]ular events. D
[normal]ecisions usually need only an estimate of the likelihood that some critical threshold for a particular impact is exceeded. 
[normal]Uncertainty expressed in this way also provides a succinct message that the public are likely to understand easily: for example, 
[normal]a hypothetical 
[normal]message that “it is likely (65 – 80%) that, 
[normal]without significant global emissions reductions
[normal], the flood risk in England will increase 
[normal]over the next 100 years
[normal]”
[normal] 
[normal]is sufficiently clear and concise to be appreciated by most of the population.  
[normal]There are 
[normal]also 
[normal]reasons to have greater confidence in 
[normal]probabilities that thr
[normal]e
[normal]sholds will be
[normal] exceeded.
[normal]The problem of uncertainty
[normal]12
[normal].
[normal] 
[normal]T
[normal]he se
[normal]cond challenge noted above concerns situations in which
[normal] firm probabilities are not available
[normal]. T
[normal]he question 
[normal]here 
[normal]is deeper than how one would communicate them if they were
[normal] available
[normal].
[normal] To discuss this point, we need to outline briefly why probabilities may not be available
[normal], and we outline 
[normal]two 
[normal]ways of dealing with
[normal] uncertainty/diversity in probabilistic estimates in the next two paragraphs
[normal]. 
[normal] 
[normal]13. One way of assessing some of the uncertainty is to allow the initial values entered into a particular model, and the parameters in the model, for each simulation, to be taken from distributions of values. Once a sufficiently large number (an “ensemble”) of forecasts has been produced, the proportion of those forecasts having some characteristic, such as showing a temperature increase of two degrees or more, is sometimes treated as an 
[normal]estimate
[normal] of the probability of that characteristic at whatever horizon for which the forecasts were made. (Significantly, more effective methods than simple counting are used in practice.)
[normal]1
[normal]4
[normal]. 
[normal]F
[normal]or some variables, predictions from different climate models diverge
[normal] considerably
[normal] but, for other variables
[normal], there is greater consistency
[normal] across such a “multi-model ensemble”
[normal].
[normal] 
[normal]The language of “confidence” is more appropriate for expressing the difference in degree of uncertainty when model forecasts are in conflict; the probability that each individual model forecast is believed realistic is also of value.
[normal]  
[normal]Importantly, i
[normal]t is simply not 
[normal]the case that these
[normal] 
[normal]ensembles encapsulate
[normal] all types of uncertainty involved in forecasts
[normal].
[normal] 
[normal]T
[normal]hus
[normal],
[normal] 
[normal]they provide 
[normal]at best 
[normal]incomplete
[normal] estimates of probabilities for the outcomes of interest
[normal] 
[normal]under clearly vetted conditions 
[normal]and
[normal],
[normal] at worst
[normal],
[normal] excuses for false confide
[normal]n
[normal]ce
[normal].  
[normal]While they may
[normal] be a more 
[normal]reliable guide
[normal] than pure judgment
[normal],
[normal] just how good a guide 
[normal]they are 
[normal]remains an issue of debate
[normal],
[normal] 
[normal]even in th
[normal]e case of seasonal forecasting (
[normal]and
[normal],
[normal] of course
[normal],
[normal] we 
[normal]can 
[normal]have no relevant empirical evidence in climate
[normal] forecasting on which to assess them).
[normal]  
[normal] 
[normal]1
[normal]5
[normal]. 
[normal]The process of model-building starts with the most robust and familiar aspects of the prob
[normal]lem under consideration and then develops
[normal] 
[normal]over time 
[normal]to incorporate more and more of the less-well-understood effects.  It is therefore plausible that, as models become more advanced and more 
[normal]detail
[normal]ed information
[normal] 
[normal]is expected of the
[normal]ir
[normal] output, 
[normal]the diversity of outcomes increase. L
[normal]evels of uncertainty 
[normal]will appear to 
[normal]increase
[normal] if we have not allowed for, or worse, have suppressed, likely shortcomings in today’s models
[normal]. 
[normal] 
[normal]16
[normal]. 
[normal]There is a spectrum of confidence 
[normal]in climate predictions. These range 
[normal]from the globally-averaged quantities in which we have reasonable confidence
[normal],
[normal] to regional impacts for which we have a very high level of uncertainty
[normal], all hinging on the assumption the models have high fidelity. 
[normal]  Initial value ensembles and 
[normal]perturbed 
[normal]parameter ensembles 
[normal]show us 
[normal]the diversity in current 
[normal]model
[normal] behaviour
[normal].  Multi-model ensembles such as the IPCC use also help 
[normal]us 
[normal]to understand possible dependence of results on model structure, but the number and independence of these models are very limited
[normal] and 
[normal]t
[normal]he IPCC itself does not interpret t
[normal]he ensembles to reflect the pro
[normal]bable range of reality
[normal] 
[normal]even for global mean temperature
[normal]. 
[normal]  
[normal]Thus, we should not communicate the diversity of model simulations as if it were the uncertainty in our future, unless we believe that diversity reflects a probability that genuinely provides a basis for action
[normal]; this is not the case for today’s models
[normal]. 
[normal]W
[normal]e can
[normal], however, 
[normal]say that we have more confidence in results which are common to all model structures
[normal], while noting that the fact these models share known common errors limits our ability to quantify this increased confidence. 
[normal]I
[normal]n such cases, we need to inform the public of our confidence in the scientific results
[normal], not just 
[normal]the 
[normal]uncertainty
[normal] from today’s modelling experiments
[normal]. How should this be done?
[normal]17. 
[normal]For the IPCC fifth 
[normal]assessment 
[normal]report, lead authors are told that: “confidence should not be interpreted probabilistically” (page 3).
[normal] Instead, it should reflect the type, amount, quality and consistency of evidence and the extent to which different experts agree on it.  These should be assessed and used to assign confidence into one of five categories (very low, low, medium, high, very high). 
[normal]18. 
[normal]Although criteria for correct assignments are suggested, it is clear that confidence assessment is a highly subjective process
[normal], reflecting the subjective nature of the uncertainty itself
[normal].
[normal]  In other words, 
[normal]the uncertainty
[normal] that we have is concerned with 
[normal]our own lack of knowledge about the system, not a property of the system itself.
[normal] There are two levels of 
[normal]this uncertainty:
[normal] the uncertainty reflected in the diversity of our current models, and the chance that, for a particul
[normal]a
[normal]r time and forecast, our 
[normal]c
[normal]urrent models are 
[normal](un)
[normal]able to provide realistic simulations at all.
[normal] Our confidence assessments need to reflect both of these.
[normal]Recommendations
[normal]19. To increase public trust in the results of climate science, we recommend that the uncertainties inherent in climate forecasts be made 
[normal]publicly 
[normal]explicit
[normal] and discussed openly.
[normal]20. In cases where 
[normal]climate 
[normal]scientists consider that these uncertainties can be legitimately expressed as probabilities, we suggest that use of a dual verbal/numerical format for expressing uncertainty (e.g., ‘likely (65-80%)’).
[normal]21. 
[normal]I
[normal]n addition to that, or as the sole measure i
[normal]n cases where climate scientists consider that t
[normal]he uncertainties 
[normal]cannot be legitimately expressed 
[normal]as proba
[normal]bilities, we propose that the verbal 
[normal]scale developed by the Intergovernmental Panel on Climate Change for their 
[normal]Fifth Asses
[normal]sment Report
[normal] is
[normal] used to communicate confidence in forecasts 
[normal]and the science underlying them
[normal].
[normal]22. Greater public trust in results of climate science is more likely to develop if the pu
[normal]blic understand 
[normal]better the processes of scientific research and the use of computer models in such endeavours; 
[normal]in 
[normal]particular the characteristics of different types of uncertainty
[normal] in climate science
[normal]. 
[normal]We recommend that the media give greater attention to this issue
[normal], more clearly distinguishing what is near certain and what is uncertain and likely to change
[normal];
[normal] as well as how the two can live side by side in the same field of scientific research without contradiction
[normal].
[normal]April 2013
[normal]&#xa0;
[normal]