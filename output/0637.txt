&#xa0;&#xa0;

[[[Joint written evidence submitted by ]]]

[[[Adam J L Harris, Nigel Harvey, Leonard A Smith, ]]]

[[[David A Stainforth, Erica Thomp]]]

[[[son ]]]

[[[(CLC018)]]]Executive SummaryOur main aim is to address the following point in the call for evidence from the Science and Technology Committee inquiry into “Climate: Public understanding and policy implications”:We begin by discussing the wider context of the challenges and relevance of public engagement with science and with scientific uncertainties. We go on to comment on the way uncertainties in future climate are often characterised today. We end with discussion of some significant issues regarding the character of climate prediction uncertainties and their relevance for public understanding.To increase public trust in the results of climate science, we recommend that the uncertainties inherent in climate forecasts be made explicit. In cases where these uncertainties can be legitimately expressed as probabilities, we suggest that use of a dual verbal/numerical format for expressing uncertainty (e.g., ‘likely (65-80%)’) will enhance communicative effectiveness.  We also propose use of the verbal confidence scale developed by the Intergovernmental Panel on Climate Change for their Fifth Assessment Report, especially important in cases where there are underlying problems in expressing the uncertainties as probabilities. Finally, we see the media as having a role in helping the public understand how forecasts are generated from models of the climate and, more particularly, the degree to which diversity in outputs from these models reflects uncertainty in those forecasts. Brief Introduction to SubmittersThis evidence has been collated by five people with relevant expertise. Adam Harris is a Lecturer, in the Department of Cognitive, Perceptual and Brain Sciences at UCL. He has published research on the communication of environmental risks within the context of climate change. Nigel Harvey is Professor of Judgment and Decision Research at UCL and Visiting Fellow in the Department of Statistics at LSE. He has published research on the role of judgment in forecasting, on determinants of trust, and on the quality of subjective estimates of probability. Leonard Smith is Research Professor in the Department of Statistics at LSE and Director of the Centre for the Analysis of Time Series. He is also a Programme Director at the Centre for Climate Change Economics and Policy and Senior Research Fellow at Pembroke College, Oxford. He has published widely on the role of models in weather and climate research and on the interpretation of ensemble forecasts derived from such models. David Stainforth is Senior Research Fellow at the Grantham Research Institute on Climate Change and the Environment at LSE, where he is on the lead staff of the Global Response Strategies Research Programme. He co-founded and was chief scientist on the climateprediction.net project, the world’s largest climate modelling experiment. His work focusses on how robust and useful information can be extracted from climate modelling experiments and on how to relate climate science to real-world decision making so that it will be of value to industry, policy makers and wider society. Erica Thompson is a Research Officer in the Centre for the Analysis of Time Series, working on the Munich Re Programme evaluating climate risks and opportunities.&#xa0;&#xa0;Basis for recommendationsIntroduction1. Policy makers are frequently considered the primary consumers of statements about future climate.  Public support for policies, however, is critical to ensure their smooth implementation: they will be more effective if people comply willingly with them. Such support depends on people’s trust in both policy makers and their scientific advisors. That trust, in turn, depends both on perceived competence and on perceived benevolence of motives: policy makers and their advisors may be mistrusted either because they are seen as well-meaning but incompetent or as competent but with a hidden agenda that favours vested interests. 2. Greater trust in scientific estimates of climate change is likely not just to increase support for government policies addressing it but also to lead to an increase in personal behaviours that may mitigate it. For example, people may reduce car use or increase home insulation.3. Previous problems with policies based on uncertain science have arisen because people were given the impression that our knowledge was more certain than it was (BSE) or because, consequent on this, they did not trust assurances that our knowledge had a firm basis when it actually did so (MMR). The lessons we need to learn from these events are that 1) we need to make levels of uncertainty associated with the science underpinning our policies explicit and 2) we need to make our rationale for those uncertainty estimates explicit. We need to do these things to increase trust. As Paul Slovic has pointed out, transparency (not withholding information) is a factor that increases trust: it reassures people that there is no hidden agenda.4. Climate modellers have developed a number of techniques that allow them to produce estimates of uncertainties associated with different degrees of climate change under specified assumptions.  Climate scientists do not, however, always consider the diversity of their models to reflect probabilities expressing uncertainties associated with their forecasts.  Moreover, a model-based estimate of probability is always incomplete without an accompanying (quantitative) estimate of our confidence that the model is able to provide meaningful information. To put this the other way round, as is often the most useful way of presenting it, what is the probability that the model is misinformative?5. It is also important to recognise that different aspects of the climate problem have different characteristics of uncertainty. Basic science is sufficient to know that continued anthropogenic emissions of greenhouse gases at the current rate will lead to a warmer planet and climate disruption with severe negative consequences for society. The details of how that will play out and the detailed national benefits of any particular mitigation policy is still an area with substantial and conceptual uncertainties.6. In summary, people must feel that current knowledge is reasonably sound, that its limitations are being honestly communicated to them, and that policy makers take account of these limitations. This, in turn, implies that the public should somehow be made aware of the uncertainty estimates associated with climate forecasts (including the uncertainty in so-called probabilistic projections) and they should be helped to understand how we can have confidence in the big picture without being able to predict the details. We now turn to problems associated with communicating this uncertainty.7. Climate models have been used to produce “probability estimates” but these estimates come from different methods with different assumptions and often different models. We do not yet have methods to provide robust probabilities for many detailed aspects of future climate change. Thus, there are two distinct challenges: one is to communicate probabilities when we have them; the other is to communicate uncertainty when such probabilities are not available. Communication of probabilistic information8. To address the first challenge, the Intergovernmental Panel on Climate Change (IPCC) adopted a policy of providing an explicit mapping between seven verbal terms and seven ranges of probability. The probabilities could then be expressed in verbal terms. Thus, in the IPCC mapping, ‘virtually certain’ refers to probabilities in the range 99 – 100% whereas ‘likely’ refers to probabilities in the range 66 – 100%. This approach allowed results from experts who disagreed about precise numerical probabilities to be expressed in the same verbal terms. Thus, an expert who associated a probability of 70% with a particular future event and another expert who associated a probability of 90% with that event could both be said to assess the event as being ‘likely’.9. However, there are a number of problems associated with translating probabilities into verbal terms. First, terms, such as ‘virtually certain’ and ‘very likely’, are intuitively matched up to different ranges of probability by different people. Thus, ’very likely’ may refer to probabilities in the range 85 – 95% for one person but to 70 – 85% for another. Of particular relevance here, recent studies have shown that the particular mapping adopted by the IPCC is not consistent with how people naturally use the probability terms included in the IPCC mapping. Furthermore, the range of probabilities associated with particular verbal expressions depends on various other factors, such as the seriousness and the expected frequency (base rate) of the event to which the term refers. Thus expressions referring to more severe outcomes are interpreted as denoting a higher probability.10. Given these problems with translating probability information into words, why should it not be communicated in its raw numerical form? There are two arguments against this. First, a single probability may give the impression that uncertainty estimates are more accurate than they are. This can be countered by using numerical ranges of probabilities (e.g., 70 – 85%). Second, it is sometimes claimed that residents of the United Kingdom cannot easily understand probabilities and that information (e.g., weather forecasts) should therefore not be given probabilistically. To address this, both verbal terms and a range of numerical probabilities could be used to express the uncertainty associated with a particular event. Thus, an event could be labelled as ‘likely (65 – 80%)’. Providing information in this dual form would supply probability information to those who can benefit from it without removing the verbal expressions of uncertainty from those who cannot.11. People need not be given probabilities (or ranges of probabilities) for particular events. Decisions usually need only an estimate of the likelihood that some critical threshold for a particular impact is exceeded. Uncertainty expressed in this way also provides a succinct message that the public are likely to understand easily: for example, a hypothetical message that “it is likely (65 – 80%) that, without significant global emissions reductions, the flood risk in England will increase over the next 100 years” is sufficiently clear and concise to be appreciated by most of the population.  There are also reasons to have greater confidence in probabilities that thresholds will be exceeded.The problem of uncertainty12. The second challenge noted above concerns situations in which firm probabilities are not available. The question here is deeper than how one would communicate them if they were available. To discuss this point, we need to outline briefly why probabilities may not be available, and we outline two ways of dealing with uncertainty/diversity in probabilistic estimates in the next two paragraphs.  13. One way of assessing some of the uncertainty is to allow the initial values entered into a particular model, and the parameters in the model, for each simulation, to be taken from distributions of values. Once a sufficiently large number (an “ensemble”) of forecasts has been produced, the proportion of those forecasts having some characteristic, such as showing a temperature increase of two degrees or more, is sometimes treated as an estimate of the probability of that characteristic at whatever horizon for which the forecasts were made. (Significantly, more effective methods than simple counting are used in practice.)14. For some variables, predictions from different climate models diverge considerably but, for other variables, there is greater consistency across such a “multi-model ensemble”. The language of “confidence” is more appropriate for expressing the difference in degree of uncertainty when model forecasts are in conflict; the probability that each individual model forecast is believed realistic is also of value.  Importantly, it is simply not the case that these ensembles encapsulate all types of uncertainty involved in forecasts. Thus, they provide at best incomplete estimates of probabilities for the outcomes of interest under clearly vetted conditions and, at worst, excuses for false confidence.  While they may be a more reliable guide than pure judgment, just how good a guide they are remains an issue of debate, even in the case of seasonal forecasting (and, of course, we can have no relevant empirical evidence in climate forecasting on which to assess them).   15. The process of model-building starts with the most robust and familiar aspects of the problem under consideration and then develops over time to incorporate more and more of the less-well-understood effects.  It is therefore plausible that, as models become more advanced and more detailed information is expected of their output, the diversity of outcomes increase. Levels of uncertainty will appear to increase if we have not allowed for, or worse, have suppressed, likely shortcomings in today’s models.  16. There is a spectrum of confidence in climate predictions. These range from the globally-averaged quantities in which we have reasonable confidence, to regional impacts for which we have a very high level of uncertainty, all hinging on the assumption the models have high fidelity.   Initial value ensembles and perturbed parameter ensembles show us the diversity in current model behaviour.  Multi-model ensembles such as the IPCC use also help us to understand possible dependence of results on model structure, but the number and independence of these models are very limited and the IPCC itself does not interpret the ensembles to reflect the probable range of reality even for global mean temperature.   Thus, we should not communicate the diversity of model simulations as if it were the uncertainty in our future, unless we believe that diversity reflects a probability that genuinely provides a basis for action; this is not the case for today’s models. We can, however, say that we have more confidence in results which are common to all model structures, while noting that the fact these models share known common errors limits our ability to quantify this increased confidence. In such cases, we need to inform the public of our confidence in the scientific results, not just the uncertainty from today’s modelling experiments. How should this be done?17. For the IPCC fifth assessment report, lead authors are told that: “confidence should not be interpreted probabilistically” (page 3). Instead, it should reflect the type, amount, quality and consistency of evidence and the extent to which different experts agree on it.  These should be assessed and used to assign confidence into one of five categories (very low, low, medium, high, very high). 18. Although criteria for correct assignments are suggested, it is clear that confidence assessment is a highly subjective process, reflecting the subjective nature of the uncertainty itself.  In other words, the uncertainty that we have is concerned with our own lack of knowledge about the system, not a property of the system itself. There are two levels of this uncertainty: the uncertainty reflected in the diversity of our current models, and the chance that, for a particular time and forecast, our current models are (un)able to provide realistic simulations at all. Our confidence assessments need to reflect both of these.Recommendations19. To increase public trust in the results of climate science, we recommend that the uncertainties inherent in climate forecasts be made publicly explicit and discussed openly.20. In cases where climate scientists consider that these uncertainties can be legitimately expressed as probabilities, we suggest that use of a dual verbal/numerical format for expressing uncertainty (e.g., ‘likely (65-80%)’).21. In addition to that, or as the sole measure in cases where climate scientists consider that the uncertainties cannot be legitimately expressed as probabilities, we propose that the verbal scale developed by the Intergovernmental Panel on Climate Change for their Fifth Assessment Report is used to communicate confidence in forecasts and the science underlying them.22. Greater public trust in results of climate science is more likely to develop if the public understand better the processes of scientific research and the use of computer models in such endeavours; in particular the characteristics of different types of uncertainty in climate science. We recommend that the media give greater attention to this issue, more clearly distinguishing what is near certain and what is uncertain and likely to change; as well as how the two can live side by side in the same field of scientific research without contradiction.April 2013&#xa0;