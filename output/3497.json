"&#xa0;\nWritten evidence submitted by the National Centre for Cyberstalking Research (NCCR) [OLS0091]&#xa0;Background:\n1\n.\n              \nThe Internet is an excellent vehicle that allows enhanced communications enabling a more cohesive society and more productive business world.  However, there a\nre a number of serious issues that have arisen in this communications network, and further legislation and regulation are needed to ensure the safety of a number of groups.  \nThe National Centre for Cyberstalking Research (NCCR) \nw\nas established to address t\nhe need for research and analysis \ninto the motivations, means, impact and investigation of cyberstalking\n.\n  This submission to the call for responses to the \nCulture, Media and Sport Committee inquiry into Online Safety\n predominantly relates to matters aroun\nd online grooming and stalking.  Professor Maple has also contributed to the submission by the BCS.\nHow best to protect minors from accessing adult content\n2\n.\n              \nIt should be recognised that there is a fundamental difficulty in current computer systems\n \naccess\n.  That is, that a user usually presents an identity (username) and access credential (password, biometric, token or other).  Whoever possesses this information can, in general, access authorised information.  Given the acceptance that there is adult conte\nnt on the Internet, any minor with appropriate credentials could access that content: those minors that are suitably determined will be able to access material intended only for adults.  Society must make it virtually impossible for minors to unintentional\nly access adult content, and difficult for minors to access adult content that they have actively sought.\n3\n.\n              \nThere is a need for clarity about the role of parents in “parental controls”.  Many parents are unaware of what is their responsibility and what is\n the responsibility others.  Many families that we have met, even with “filters” and “controls” in place, are sure their children have access to material not intended for minors.  They are unaware of what they can do to prevent such access.\n4.\n              \nThere is a p\nroblem regarding the understanding, by children, of what is \nadult content. \nWhilst with games, films and even music there may be some classification of the content, that does not exist for a great deal of information and content on the Internet.  Furthermor\ne some “adult” \n(and indeed illegal) \ncontent is actually \nproduced\n by children (in the form of sexting for example)\n.  The Internet Watch Foundation \n(IWF) \nhas stated that 12,224 images, apparently created by minors, were reported in just four weeks. \n5.\n              \nConte\nnt (illegal, adult or otherwise) can spread around the internet very rapidly and so removing material from one site may have minimal impact if the material has already been harvested and reposted to another site.\n6.\n              \nWhile awareness is important, education \nis also necessary.  The technical knowledge of children is very impressive, but there is a need to educate children on the impact of technology and the management of information in the unregulated space that is the Internet.  There is good guidance availab\nle through initiatives such as Safer Internet and Get Safe Online.\n \n A particular area that requires special attention in any education programme is the spread and the permanence of material put online.\n7.\n              \nIt is not only children that require educating, but\n also parents.  Parents use the Internet in different ways to children and therefore may not understand how best to protect their children.  It is unlikely as many parents have seen Chat Roulette as children have.  Children consider email outdated and use \nYoutube not only to access videos but as a discussion medium.  It would be positive for schools to be involved in the bringing together or parents and children.  Topics that could be discussed include the management of online relationships, appropriate com\nmunications (sending and receiving) and what to do if children are concerned by interactions online.  \nFiltering out extremist material, including images of child abuse and material intended to promote terrorism or other acts of violence\n8.\n              \nThere are two \nmain types of method that can be employed to filter material; material can be filtered by automatic or manual means.  The former relies on sufficiently sophisticated algorithms do detect unwanted material and the latter relies on people in the community no\ntifying an authority of the existence of unwanted material.  Such material is then removed, and an identifier placed on a blacklist so that the material cannot enter a site again.\n9.\n              \nThe difficulties in blacklisting material should be recognised.  Automate\nd tools are not yet sufficiently advanced to perform blacklisting without some human control.  An Irish study considering flitering within a school setting found that 50% of schools reported that filtering occasionally blocked valid educational sites, whil\ne 20% of schools reported that it regularly did so. (\nhttp://tnc2007.terena.org/programme/presentations/show4fa0.html\n).  Hence it is difficult to rely on content based automated\n tools alone.  Furthermore, what should be restricted and what should pass through a filter is often a judgement call and so some level of human intervention is likely to be required.  \n10.\n              \nBlacklisting is an important activity but does come with a cost, b\noth financial and non-financial, that must be met.  Currently, for example, the Internet Watch Foundation, is largely funded by ISPs.  This model would need to continue, but it may be possible to consider other routes for funding filtering technologies in \nthe general sense.\n  The value of the time of the volunteers that report material to the IWF is not insignificant, and must be understood.  It is vital that reporting of unwanted material is a simple and transparent process.  The True Vision initiative for \nhate crime reporting is an excellent example of a simple and transparent reporting mechanism, but it needs to be more widely publicised.\n11.\n              \nConsideration could be given to requesting the Internet Watch Foundation to widen its remit – this could have the b\nenefit of a one-stop reporting centre.\nPreventing abusive or threa\ntening comments on social media\n12.\n              \nThe issue of abusive or threatening comments on social media has clearly been one that has disturbed society of late and gained major press coverage.  \nIn \nparticular, the case on Twitter of \nC\naroline Criado-Perez received major attention and highlighted this negative side of the new communications environment we live in.  In response to the case, Twitter did state it would trial a “Report Abuse” button.  Ther\ne is no such button available on Twitter at this time and this resistance by the social media companies to protect its users and provide an avenue for reporting requires addressing.  Research undertaken by the NCCR has shown that people do not know where t\no report cyberstalkng abuse, or who should be responsible.  \n13.\n              \nSome social media providers do provide a Report Abuse button, but it is important that they also provide clear guidance and support for victims of distressing communications.  These should de\ntail methods for locating support and information on how to report the incident(s).  Where possible and appropriate providers should maintain regular contact with support and criminal justice agencies.\n114.\n              \nThe Guidelines on prosecuting cases involving \ncommunications sent via social media from the Director of Public Prosecut\nions published on June 20, 2013, categorise communications as those which:\na.\n              \namount to threats of violence;\nb.\n              \ntarget an individual and amounts to harassment or stalking within the m\neaning of the Protection from Harassment Act 1997 (including two new criminal offences of stalking were added as sections 2A and 4A to the Act by the Protection of Freedoms Act 2012);\nc.\n              \namount to a breach of a court order;\nd.\n              \nthose not covered by the prov\nision above but may be considered grossly offensive, indecent, obscene or false.\n15.\n              \nThe distinction between the cases is very important to ensuring the correct legislation, if any is used in cases of threatening or abusive communications.  In particular, \ngiven the evidence of the significant impact of the first three categories (including recent evidence on the impact of cyberstalking) we must ensure that such actions are not simply considered as grossly offensive.   This would, however, be the easiest rou\nte for many stakeholders to take and therefore should be guarded against.\n16.\n              \nWithout the public having a clear understanding of the differences in these communications the problem is unlikely to diminish.  Digital Natives have embraced technology but unfo\nrtunately without appropriate training and education they struggle to understand the social norms of internet communication and behaviour.  Education surrounding the appropriate use of these new communications media will have an important role to play in c\nombating the problem.\n17.\n              \nThere is a clear issue around anonymity and perceived anonymity (as well as untraceability) in social media.  \na.\n              \nIn cases where senders of malicious communications have anonymity and (practical) untraceability there can be diffic\nulty in bringing justice and technological and legl changes may be needed. \nb.\n              \nIn cases where senders have a (mistaken) perception of anonymity or untraceability they may display fewer inhibitions and feel beyond reproach.  It is important that all those t\nhat can assist in making communications more traceable, particularly by giving up log information, do so fully and readily when requested by those in the criminal justice system.  The recent changes to the Protection from Harassment Act 1997 do give police\n further powers and this is a welcome change.\nc.\n              \nWhere receivers of abusive messages perceive (rightly or wrongly) anonymity or untraceability of the senders they may feel there is little point in reporting the communication, even when it has significant i\nmpact.\n18.\n              \nIt is important all stakeholders consider the vulnerability of the victim in cases of abusive or threatening messages.\n&#xa0;\nSeptember 2013\n&#xa0;\n&#xa0;\n"