"&#xa0;\n&#xa0;\nJoint written evidence submitted by Adam J L Harris, Nigel Harvey, Leonard A Smith, David A Stainforth, Erica Thompson (CLC018)Executive Summary\nOur main aim is to address the following point in the call for evidence from the Science and Technology Committee inquiry into “Climate: Public understanding and policy implications”:\nWe begin by discussing the wider context of the challenges and relevance of public engagement with science and with scientific uncertainties. We go on to comment on the way uncertainties in future climate are often characterised today. \nWe end\n with \ndiscussion of \nsome significant issues regarding the character of climate prediction uncertainties and their relevance for public understanding.\nTo increase public trust in the results of climate science, w\ne recommend that the uncertainties inherent in climate forecasts be made explicit. In cases where these uncertainties can be legitimately expressed as\n probabilities, we suggest that use of a dual verbal/numerical format for expressing uncertainty (e.g., \n‘\nlikely (65-80%)\n’\n) will enhance communicative effectiveness. \n We also \npropose \nuse of the verbal confidence scale developed by the \nIntergovernmental Panel on Climate Change for their \nFifth Assess\nment Report\n, especially important i\nn cases where there are underlying problems in expressing the uncertainties as probabilities\n. Finally, we see the media as having a role in helping the public understand how forecasts are generated from models of the climate and, more particularly, \nthe degree to which\n diversity in outputs from these models reflects uncertainty in those forecasts.\n \nBrief Introduction\n to Submitters\nThis evidence has been collated by f\nive\n people with relevant expertise. Adam Harris is a Lecturer, in the Department of Cognitive, Perceptual and Brain Sciences at UCL. He has published research on the communication of environmental risks within the context of climate change. Nigel Harvey is Professor of Judgment and Decision Research at UCL and Visiting Fellow in the Department of Statistics at LSE. \nHe has published research on the role of judgment in forecasting\n, on determinants of trust,\n and on the quality of subjective estimates of probability. Leonard Smith is Research Professor in the Department of Statistics at LSE and Director of the Centre for\n the Analysis of\n Time Series. He is also a Programme Director at the Centre for Climate Change Economics and Policy\n and Senior Research Fellow at Pembroke College, Oxford\n. He has published widely on the \nrole of models in weather and climate research and on the interpretation of ensemble forecasts derived from such models. David Stainforth is Senior Research Fellow at the Grantham Research Institute on Climate Change and the Environment at LSE\n, where he is on the lead staff of the Global Response Strategies Research Programme\n. He co-founded and was chief scientist on the climateprediction.net project, the world’s largest climate modelling experiment. \nHis work focusses on how robust and useful information can be extracted from climate modelling experiments and on how to relate climate science to real-world decision making so that it will be of value to industry, policy makers and wider society.\n E\nrica Thompson is a Research Officer in the Centre for the Analysis of Time Series\n, working on the Munich Re Programme evaluating climate risks and opportunities.\n&#xa0;\n&#xa0;\nBasis for recommendations\nIntroduction\n1. \nPolicy makers are frequently consider\ned\n the primary consumers of statements about future climate. \n \nP\nublic support for policies\n, however\n, is\n \ncritical\n \nto ensure their smooth implementation: they will be more effective if people comply willingly with them\n. Such support\n \ndepends on people’s trust in both policy makers and their scientific advisors. T\nhat t\nrust\n, in turn, depends \nboth \non \nperceived competence and \non \nperceived benevolence of motives: policy makers and their advisors may be mistrusted either because they are seen as well-meaning but incompetent or as competent but with a hidden agenda \nthat favours vested interests.\n \n2. Greater trust in scientific estimates of climate change is likely not just to increase support for government policies addressing it but also to lead to an increase in personal behaviours that may mitigate it. For example, people may reduce car use or increase home insulation.\n3. \nPrevious problems with policies based on uncertain science have arisen because people were given the impression that our knowledge was more certain than it was (BSE) or because, consequ\nent on this, they did not trust\n \nassurances that our knowled\nge had a firm basis when it act\nually did \nso \n(MMR). \nThe lessons we need to learn from these events are that 1) we need to make levels of uncertainty associated with the science underpinning our policies \nexplicit and 2) we need to make \nour rationale for those\n uncertainty estimates expli\ncit\n.\n \nWe need to do these things to increase trust. As Paul Slovic has pointed out, transparency (not withholding information) i\ns a\n fac\ntor\n that increases trust: it reassures people that there is no hidden agenda\n.\n4\n. C\nlimate modellers have developed a number of techniques that allow them \nto produc\ne estimates of unce\nr\ntainties\n associated with different degrees of climate change under specified assumptions. \n \nC\nlimate scientists \ndo not\n,\n \nhowever, always \nconsider \nthe diversity of their models to \nre\nf\nlect \nprobabilities \nexpressing \nuncertainties\n associated with \ntheir \nforecasts.\n  Moreover, a\n model-based estimate of probability is always incomplete without an accompanying \n(quantitative) \nestimate of our confidence that the model is able to provide meaningful information\n.\n To put this the other way round, as is often the most usefu\nl way of presenting it, what is\n \nt\nhe probability that the model is misinformative\n?\n5\n. It is also important to recognise\n that different aspects of the climate problem have different character\nistic\ns of uncertainty. Basic science is sufficient to know that continued anthrop\no\ngenic emissions of greenhouse gases at the current rate will lead to \na warmer planet and \nclimate disruption with severe negative consequences for society. The details of how that will play out and the detailed national benefits of any particular miti\ngation policy is still an area \nwith substantial and conceptual uncertainties.\n6\n. \nIn summary, \npeople\n must feel that current knowledge\n is reasonably sound, \nthat its limitations are being honestly communicated to them\n, and that policy makers take account of these limitations. This, in turn, implies that the public should somehow be made aware of the uncertainty estimates associated with climate forecasts\n \n(\nincluding the uncertainty in so\n-\ncalled probabilistic projections\n) \nand \nthey should be helped to understand how we\n can have confidence in the big picture without being able to predict the details. \nWe now turn to problems associated with communicating \nthis \nuncertainty.\n7. Climate models have been used to produce “probability estimates” but \nthese \nestimate\ns\n come from different \nmethods with different \nassumption\ns\n and \noften \ndifferent models\n. We do not yet have methods to provide robust probabilities for many detailed aspects of \nfuture \nclimate change.\n Thus, there are two distinct challenges: one is to communicate probabilities when we have them; the other is to communicate uncertainty when such probabilities are not available. \nCommunication of probabilistic \ninformation\n8\n. \nTo address th\ne first challenge\n, t\nhe Intergovernmental Panel on Climate Change (IPCC) adopted a policy of providing an explicit mapping between seven verbal terms and seven ranges of probability. \nThe probabilities could then be expressed in verbal terms. \nThus,\n in the IPCC mapping\n, ‘virtually certain’ refer\ns\n to probabilities in the range 99 – 100% whereas ‘\nlikely\n’\n \nrefer\ns\n \nto probabilities in the range 66\n –\n 100\n%. \nThis approach allowed results from experts who disagreed about precise numerical probabilities to be expressed in the same verbal terms. Thus, an expert who associated a probability of 70% with a particular future event and another expert who associated a probability of 90% with that event could both be said \nto assess the event as being ‘likely’.\n9\n. However, there are \na number of \nproblems \nassociated \nwith translating probabilities into verbal terms. \nFirst, t\nerms, such as ‘virtually certain’ and ‘very likely’, are \nintuitively \nmatched up to different ranges of probability by different people. Thus, ’very likely’ may refer to probabilities in the range 85 – 95% for one person but to 70 – 85% for another. \nOf particular relevance here\n, recent studies have shown that the particular mapping adopted by the IPCC is not consistent with how people naturally use the probability terms included in the IPCC mapping. \nFurthermore\n, the range of probabilities associated with particular verbal ex\npressions depends on various \nother factors, such as the seriousness and the expected frequency \n(base rate) \nof the event to which the term refers\n. Thus\n \ne\nxpressions referring to more severe outcomes are interpreted as denoting a higher probability\n.\n10\n. Given these problems with translating probability information into words, why should it not be communicated in its raw numerical form? There are two arguments against this. First, \na single probability may give the impression that uncertainty estimates are more accurate than they are. This can be countered by using numerical ra\nnges of probabilities (e.g., 70 – \n85%). Second, it is sometimes \nclaimed that residents of the United Kingdom cannot easily understand probabilities and that information (e.g., weather forecasts) should therefore not be given probabilistically.\n To address this, both verbal terms and a range of numerical probabilities could be used to express the uncertainty associated with a particular event. Thus, an event could be labelled as ‘likely (65 – 80%)’. Providing information in this dual form would supply probability information to those who can benefit from it without removing the verbal expressions of uncertainty from those who cannot.\n11\n. \nPeople need not be given probabilities (or ranges of probabilities) for partic\nular events. D\necisions usually need only an estimate of the likelihood that some critical threshold for a particular impact is exceeded. \nUncertainty expressed in this way also provides a succinct message that the public are likely to understand easily: for example, \na hypothetical \nmessage that “it is likely (65 – 80%) that, \nwithout significant global emissions reductions\n, the flood risk in England will increase \nover the next 100 years\n”\n \nis sufficiently clear and concise to be appreciated by most of the population.  \nThere are \nalso \nreasons to have greater confidence in \nprobabilities that thr\ne\nsholds will be\n exceeded.\nThe problem of uncertainty\n12\n.\n \nT\nhe se\ncond challenge noted above concerns situations in which\n firm probabilities are not available\n. T\nhe question \nhere \nis deeper than how one would communicate them if they were\n available\n.\n To discuss this point, we need to outline briefly why probabilities may not be available\n, and we outline \ntwo \nways of dealing with\n uncertainty/diversity in probabilistic estimates in the next two paragraphs\n. \n \n13. One way of assessing some of the uncertainty is to allow the initial values entered into a particular model, and the parameters in the model, for each simulation, to be taken from distributions of values. Once a sufficiently large number (an “ensemble”) of forecasts has been produced, the proportion of those forecasts having some characteristic, such as showing a temperature increase of two degrees or more, is sometimes treated as an \nestimate\n of the probability of that characteristic at whatever horizon for which the forecasts were made. (Significantly, more effective methods than simple counting are used in practice.)\n1\n4\n. \nF\nor some variables, predictions from different climate models diverge\n considerably\n but, for other variables\n, there is greater consistency\n across such a “multi-model ensemble”\n.\n \nThe language of “confidence” is more appropriate for expressing the difference in degree of uncertainty when model forecasts are in conflict; the probability that each individual model forecast is believed realistic is also of value.\n  \nImportantly, i\nt is simply not \nthe case that these\n \nensembles encapsulate\n all types of uncertainty involved in forecasts\n.\n \nT\nhus\n,\n \nthey provide \nat best \nincomplete\n estimates of probabilities for the outcomes of interest\n \nunder clearly vetted conditions \nand\n,\n at worst\n,\n excuses for false confide\nn\nce\n.  \nWhile they may\n be a more \nreliable guide\n than pure judgment\n,\n just how good a guide \nthey are \nremains an issue of debate\n,\n \neven in th\ne case of seasonal forecasting (\nand\n,\n of course\n,\n we \ncan \nhave no relevant empirical evidence in climate\n forecasting on which to assess them).\n  \n \n1\n5\n. \nThe process of model-building starts with the most robust and familiar aspects of the prob\nlem under consideration and then develops\n \nover time \nto incorporate more and more of the less-well-understood effects.  It is therefore plausible that, as models become more advanced and more \ndetail\ned information\n \nis expected of the\nir\n output, \nthe diversity of outcomes increase. L\nevels of uncertainty \nwill appear to \nincrease\n if we have not allowed for, or worse, have suppressed, likely shortcomings in today’s models\n. \n \n16\n. \nThere is a spectrum of confidence \nin climate predictions. These range \nfrom the globally-averaged quantities in which we have reasonable confidence\n,\n to regional impacts for which we have a very high level of uncertainty\n, all hinging on the assumption the models have high fidelity. \n  Initial value ensembles and \nperturbed \nparameter ensembles \nshow us \nthe diversity in current \nmodel\n behaviour\n.  Multi-model ensembles such as the IPCC use also help \nus \nto understand possible dependence of results on model structure, but the number and independence of these models are very limited\n and \nt\nhe IPCC itself does not interpret t\nhe ensembles to reflect the pro\nbable range of reality\n \neven for global mean temperature\n. \n  \nThus, we should not communicate the diversity of model simulations as if it were the uncertainty in our future, unless we believe that diversity reflects a probability that genuinely provides a basis for action\n; this is not the case for today’s models\n. \nW\ne can\n, however, \nsay that we have more confidence in results which are common to all model structures\n, while noting that the fact these models share known common errors limits our ability to quantify this increased confidence. \nI\nn such cases, we need to inform the public of our confidence in the scientific results\n, not just \nthe \nuncertainty\n from today’s modelling experiments\n. How should this be done?\n17. \nFor the IPCC fifth \nassessment \nreport, lead authors are told that: “confidence should not be interpreted probabilistically” (page 3).\n Instead, it should reflect the type, amount, quality and consistency of evidence and the extent to which different experts agree on it.  These should be assessed and used to assign confidence into one of five categories (very low, low, medium, high, very high). \n18. \nAlthough criteria for correct assignments are suggested, it is clear that confidence assessment is a highly subjective process\n, reflecting the subjective nature of the uncertainty itself\n.\n  In other words, \nthe uncertainty\n that we have is concerned with \nour own lack of knowledge about the system, not a property of the system itself.\n There are two levels of \nthis uncertainty:\n the uncertainty reflected in the diversity of our current models, and the chance that, for a particul\na\nr time and forecast, our \nc\nurrent models are \n(un)\nable to provide realistic simulations at all.\n Our confidence assessments need to reflect both of these.\nRecommendations\n19. To increase public trust in the results of climate science, we recommend that the uncertainties inherent in climate forecasts be made \npublicly \nexplicit\n and discussed openly.\n20. In cases where \nclimate \nscientists consider that these uncertainties can be legitimately expressed as probabilities, we suggest that use of a dual verbal/numerical format for expressing uncertainty (e.g., ‘likely (65-80%)’).\n21. \nI\nn addition to that, or as the sole measure i\nn cases where climate scientists consider that t\nhe uncertainties \ncannot be legitimately expressed \nas proba\nbilities, we propose that the verbal \nscale developed by the Intergovernmental Panel on Climate Change for their \nFifth Asses\nsment Report\n is\n used to communicate confidence in forecasts \nand the science underlying them\n.\n22. Greater public trust in results of climate science is more likely to develop if the pu\nblic understand \nbetter the processes of scientific research and the use of computer models in such endeavours; \nin \nparticular the characteristics of different types of uncertainty\n in climate science\n. \nWe recommend that the media give greater attention to this issue\n, more clearly distinguishing what is near certain and what is uncertain and likely to change\n;\n as well as how the two can live side by side in the same field of scientific research without contradiction\n.\nApril 2013\n&#xa0;\n"