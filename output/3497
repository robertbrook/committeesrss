&#xa0;
defaultWritten evidence submitted by the defaultNational Centre for defaultCyberstalkingdefault Research (NCCR)default [OLS0091]default&#xa0;defaultBackground:
default1
default.
default              
defaultThe Internet is an excellent vehicle that allows enhanced communications enabling a more cohesive society and more productive business world.  However, there a
defaultre a number of serious issues that have arisen in this communications network, and further legislation and regulation are needed to ensure the safety of a number of groups.  
defaultThe National Centre for Cyberstalking Research (NCCR) 
defaultw
defaultas established to address t
defaulthe need for research and analysis 
defaultinto the motivations, means, impact and investigation of cyberstalking
default.
default  This submission to the call for responses to the 
defaultCulture, Media and Sport Committee inquiry into Online Safety
default predominantly relates to matters aroun
defaultd online grooming and stalking.  Professor Maple has also contributed to the submission by the BCS.
defaultHow best to protect minors from accessing adult content
default2
default.
default              
defaultIt should be recognised that there is a fundamental difficulty in current computer systems
default 
defaultaccess
default.  That is, that a user usually presents an identity (username) and access credential (password, biometric, token or other).  Whoever possesses this information can, in general, access authorised information.  Given the acceptance that there is adult conte
defaultnt on the Internet, any minor with appropriate credentials could access that content: those minors that are suitably determined will be able to access material intended only for adults.  Society must make it virtually impossible for minors to unintentional
defaultly access adult content, and difficult for minors to access adult content that they have actively sought.
default3
default.
default              
defaultThere is a need for clarity about the role of parents in “parental controls”.  Many parents are unaware of what is their responsibility and what is
default the responsibility others.  Many families that we have met, even with “filters” and “controls” in place, are sure their children have access to material not intended for minors.  They are unaware of what they can do to prevent such access.
default4.
default              
defaultThere is a p
defaultroblem regarding the understanding, by children, of what is 
defaultadult content. 
defaultWhilst with games, films and even music there may be some classification of the content, that does not exist for a great deal of information and content on the Internet.  Furthermor
defaulte some “adult” 
default(and indeed illegal) 
defaultcontent is actually 
defaultproduced
default by children (in the form of sexting for example)
default.  The Internet Watch Foundation 
default(IWF) 
defaulthas stated that 12,224 images, apparently created by minors, were reported in just four weeks. 
default5.
default              
defaultConte
defaultnt (illegal, adult or otherwise) can spread around the internet very rapidly and so removing material from one site may have minimal impact if the material has already been harvested and reposted to another site.
default6.
default              
defaultWhile awareness is important, education 
defaultis also necessary.  The technical knowledge of children is very impressive, but there is a need to educate children on the impact of technology and the management of information in the unregulated space that is the Internet.  There is good guidance availab
defaultle through initiatives such as Safer Internet and Get Safe Online.
default 
default A particular area that requires special attention in any education programme is the spread and the permanence of material put online.
default7.
default              
defaultIt is not only children that require educating, but
default also parents.  Parents use the Internet in different ways to children and therefore may not understand how best to protect their children.  It is unlikely as many parents have seen Chat Roulette as children have.  Children consider email outdated and use 
defaultYoutube not only to access videos but as a discussion medium.  It would be positive for schools to be involved in the bringing together or parents and children.  Topics that could be discussed include the management of online relationships, appropriate com
defaultmunications (sending and receiving) and what to do if children are concerned by interactions online.  
defaultFiltering out extremist material, including images of child abuse and material intended to promote terrorism or other acts of violence
default8.
default              
defaultThere are two 
defaultmain types of method that can be employed to filter material; material can be filtered by automatic or manual means.  The former relies on sufficiently sophisticated algorithms do detect unwanted material and the latter relies on people in the community no
defaulttifying an authority of the existence of unwanted material.  Such material is then removed, and an identifier placed on a blacklist so that the material cannot enter a site again.
default9.
default              
defaultThe difficulties in blacklisting material should be recognised.  Automate
defaultd tools are not yet sufficiently advanced to perform blacklisting without some human control.  An Irish study considering flitering within a school setting found that 50% of schools reported that filtering occasionally blocked valid educational sites, whil
defaulte 20% of schools reported that it regularly did so. (
defaulthttp://tnc2007.terena.org/programme/presentations/show4fa0.html
default).  Hence it is difficult to rely on content based automated
default tools alone.  Furthermore, what should be restricted and what should pass through a filter is often a judgement call and so some level of human intervention is likely to be required.  
default10.
default              
defaultBlacklisting is an important activity but does come with a cost, b
defaultoth financial and non-financial, that must be met.  Currently, for example, the Internet Watch Foundation, is largely funded by ISPs.  This model would need to continue, but it may be possible to consider other routes for funding filtering technologies in 
defaultthe general sense.
default  The value of the time of the volunteers that report material to the IWF is not insignificant, and must be understood.  It is vital that reporting of unwanted material is a simple and transparent process.  The True Vision initiative for 
defaulthate crime reporting is an excellent example of a simple and transparent reporting mechanism, but it needs to be more widely publicised.
default11.
default              
defaultConsideration could be given to requesting the Internet Watch Foundation to widen its remit – this could have the b
defaultenefit of a one-stop reporting centre.
defaultPreventing abusive or threa
defaulttening comments on social media
default12.
default              
defaultThe issue of abusive or threatening comments on social media has clearly been one that has disturbed society of late and gained major press coverage.  
defaultIn 
defaultparticular, the case on Twitter of 
defaultC
defaultaroline Criado-Perez received major attention and highlighted this negative side of the new communications environment we live in.  In response to the case, Twitter did state it would trial a “Report Abuse” button.  Ther
defaulte is no such button available on Twitter at this time and this resistance by the social media companies to protect its users and provide an avenue for reporting requires addressing.  Research undertaken by the NCCR has shown that people do not know where t
defaulto report cyberstalkng abuse, or who should be responsible.  
default13.
default              
defaultSome social media providers do provide a Report Abuse button, but it is important that they also provide clear guidance and support for victims of distressing communications.  These should de
defaulttail methods for locating support and information on how to report the incident(s).  Where possible and appropriate providers should maintain regular contact with support and criminal justice agencies.
default114.
default              
defaultThe Guidelines on prosecuting cases involving 
defaultcommunications sent via social media from the Director of Public Prosecut
defaultions published on June 20, 2013, categorise communications as those which:
defaulta.
default              
defaultamount to threats of violence;
defaultb.
default              
defaulttarget an individual and amounts to harassment or stalking within the m
defaulteaning of the Protection from Harassment Act 1997 (including two new criminal offences of stalking were added as sections 2A and 4A to the Act by the Protection of Freedoms Act 2012);
defaultc.
default              
defaultamount to a breach of a court order;
defaultd.
default              
defaultthose not covered by the prov
defaultision above but may be considered grossly offensive, indecent, obscene or false.
default15.
default              
defaultThe distinction between the cases is very important to ensuring the correct legislation, if any is used in cases of threatening or abusive communications.  In particular, 
defaultgiven the evidence of the significant impact of the first three categories (including recent evidence on the impact of cyberstalking) we must ensure that such actions are not simply considered as grossly offensive.   This would, however, be the easiest rou
defaultte for many stakeholders to take and therefore should be guarded against.
default16.
default              
defaultWithout the public having a clear understanding of the differences in these communications the problem is unlikely to diminish.  Digital Natives have embraced technology but unfo
defaultrtunately without appropriate training and education they struggle to understand the social norms of internet communication and behaviour.  Education surrounding the appropriate use of these new communications media will have an important role to play in c
defaultombating the problem.
default17.
default              
defaultThere is a clear issue around anonymity and perceived anonymity (as well as untraceability) in social media.  
defaulta.
default              
defaultIn cases where senders of malicious communications have anonymity and (practical) untraceability there can be diffic
defaultulty in bringing justice and technological and legl changes may be needed. 
defaultb.
default              
defaultIn cases where senders have a (mistaken) perception of anonymity or untraceability they may display fewer inhibitions and feel beyond reproach.  It is important that all those t
defaulthat can assist in making communications more traceable, particularly by giving up log information, do so fully and readily when requested by those in the criminal justice system.  The recent changes to the Protection from Harassment Act 1997 do give police
default further powers and this is a welcome change.
defaultc.
default              
defaultWhere receivers of abusive messages perceive (rightly or wrongly) anonymity or untraceability of the senders they may feel there is little point in reporting the communication, even when it has significant i
defaultmpact.
default18.
default              
defaultIt is important all stakeholders consider the vulnerability of the victim in cases of abusive or threatening messages.
default&#xa0;
defaultSeptember 2013
default&#xa0;
default&#xa0;
default