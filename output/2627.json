"&#xa0;\nWritten evidence submitted by the Open Rights Group [OLS0074]&#xa0;\n&#xa0;\nWe welcome the attention that the Committee is devoting to looking at these important issues and appreciate the opportunity to offer our views. \n&#xa0;\nThe areas covered by the Inquiry have been the subject of quite intense, heated and not always helpful or cons\ntructive debate this summer.  We hope that this Inquiry provides an opportunity for a calm and considered look at these difficult issues.\n&#xa0;\nOpen Rights Group \nhave\n done a significant amount of work on how Internet filtering works over the past few years. Tha\nt has included:\n&#xa0;\n&#xa0;\nAs an organisation focused on human rights and civil liberties in the digital age, we believe it is possible to reconcile the idea that the Internet can create greater opportunities for exercising the right to freedom of expre\nssion with a desire to tackle the problems and dangers young people now face online. \n&#xa0;\nWe do not see these as mutually exclusive aims. But we also believe that a simplistic 'restrictions' based approach to child safety will: fail on its own terms due large\nly to the limitations of filtering technology; ignore the wider social issues that young people face; and lead to overly restrictive content practices that reduce the usefulness of the Internet for everybody.\n&#xa0;1. How best to protect minors from accessing adult content&#xa0;\nIn summary, some of our key recommendations to the Government are: \n&#xa0;\n&#xa0;\nWe \nbelieve that the Department for Education response to their consultation in December 2012 arrived at\n a reasonable position – that the Government should help parents make their own decisions about what is best for their household. It has been disappointing to see the Government somewhat turn away from this and effectively mandate default-on filtering. \n&#xa0;\n&#xa0;\nChallenging the 'one click to safety' approach\n&#xa0;\nIn his summer speech on filtering and child protection the Prime Minister promised 'no more hassle of downloading filters for every device, just one click protection. One click to protect your whole home and \nto keep your children safe\n..\n' \n&#xa0;\nWe believe this is unhelpful and misleading, and runs against the advice of the previous Byron and Bailey reviews of online safety undertaken for the Government.\n&#xa0;\nThere is no switch that will keep children and young people \nsafe. If we encourage parents to believe that there is, too many will assume their job is done when they press it. That is to ignore the limitations of the filtering services and the broader issues and pressures that children and young people face online. \n&#xa0;\nStart with good evidence\n&#xa0;\nToo often anecdotes, \nunsourced\n figures or insufficiently robust evidence has been used by those in the debate about online safety. We would urge the Committee to base its findings in the best available evidence – and there is pl\nenty of good quality work, for example from the aforementioned EU Kids Online project run by Professor Sonia Livingstone - and also to be rigorous in questioning the evidence presented to it.   \n&#xa0;\nAvoiding default-on filters and \nendorse\n active choice\n&#xa0;\nThere\n is a risk that default internet filtering will move decisions about what is appropriate for families and households further out of parents' hands. The Government should instead promote an 'active choice' model that encourages parents to make their own dec\nisions about what is appropriate and what tools to use.\n&#xa0;\nWe believe the available evidence (for example, from \nthe EU Kids Online project\n) does not support an approach focused simply or primarily on filtering restrictions, and certainly not a \ndefault 'on' Internet filter. \n&#xa0;\nFiltering services are error-prone\n&#xa0;\nWe know that filtering systems often, through error or overreach, lead to the blocking of legal and legitimate content. This is a \nbyproduct\n of trying \ncategorise and filter\n such a massive v\nolume of content. We would point here, for example, to our work looking at mobile Internet filtering products (which is noted in the introduction). We found amongst others, political blogs, political campaigns, community websites, gay news sites, church gr\noups and technology news sites blocked by accident by mobile networks. \n&#xa0;\nRather than looking at \nwhether\n this is a problem, policy makers should be asking how to take this fact into account.  We are concerned that some vocal supporters of default filtering \nplay down concerns about mistaken blocking, seeing them as just pedantic quibbles\n.\n&#xa0;\nThere are a number of damaging consequences of ignoring this problem. \n&#xa0;\nFirst, website owners find it harder to figure out if their site is blocked by one of the filters. \nThis is not an easy task, as most operators of blocking services do not offer an easy way to check URLs (O2 is a notable exception – see their URL checker\n). This problem grows when a website owner is faced with a number of blocking services across the var\nious ISPs and service providers, and grows further when they face having to look at other country's policies. If we pretend that over-blocking is not a problem, there will be no effort to improve matters for affected website operators. \n&#xa0;\nSecond, our experi\nence of talking to affected website owners is that they find it hard to get their site unblocked when it is mistakenly categorised as '\nblockable\n' by a filtering service. \n&#xa0;\nService providers operating filtering services should be asked how they have assesse\nd the risks of over-blocking and whether they have taken steps to \nensue\n their deployment of filtering tries to address this. They should explain how they will ensure website owners are able to report mistakes and get problems fixed.  And service providers \nshould ensure that website owners are able to easily check whether their site has been categorised by the filtering system. \n&#xa0;\nWe would like to highlight two damaging consequences of over-blocking – on access to information, and on the economy.\n&#xa0;\nAccess to \ninformation\n&#xa0;\nFirst, it would be worrying if sites blocked by accident or through overly broad \ncateogries\n are considered collateral damage of filtering services, as this would be to deny young people access to information at the very moment when it is most \nimportant to be helping them satisfy their curiosity and interests. \n&#xa0;\nFiltering can lead to children, young people, and adults being denied access to legitimate and age-appropriate information and resources such as sexual health information and advice.\n&#xa0;\nFi\nltering that covers different age ranges and / a broadly defined set of ‘adult’ content can deny young people access to material appropriate to their development and needs. In a paper to the EU Kids Online conference in 2011, Tim Davies, \nSangeet\n \nBhullar\n an\nd Terri \nDowty\n argued that filtering can therefore restrict young people’s rights in the name of protecting them from risk – specifically “rights to freedom of expression and access to information across frontiers (Article 13, 17), rights to freedom of asso\nciation (Article 14), rights to preparation for responsible life in a free society (Article 29) and rights to protection of privacy (Article 16)”. They argue that:\n&#xa0;\n“\n...these broader rights are frequently neglected - with young people’s access to \ninformation on key topics of health, politics and sexuality limited by Internet filtering - and with a lack of critical formal and informal education supporting young people to gain the skills to live creative and responsible lives in increasingly digitall\ny mediated societies.”\n              \nAn unintended economic impact\n&#xa0;\nSecond, to give a specific example, we heard from the owner of an online gift shop that their site had been blocked by Orange's Safeguard system over Christmas last year. The site sold engraved ciga\nrette lighters, and so we assumed the filters had mistakenly categorised the site under its 'tobacco' category. Despite reporting the issue in early December it took until January to get the problem sorted and the site removed from the block list\n. \n&#xa0;\nA lar\nger business may have been able to pressure for a resolution sooner – we see no good reason that smaller businesses should be inhibited in their efforts to reach consumers online in ways larger businesses are not.\n&#xa0;\nTo build systems in which less establish\ned businesses or organisations are hampered in this way undermines one of the core benefits that the Internet offers for economic and social innovation.\n&#xa0;\nDevice based filters are preferable to network level filters\n&#xa0;\nWe urge the Committee to look seriously \nat the relative merits of network-level and device-based filters, across different contexts. We have previously written about some of the benefits and problems of each, and refer the Committee to this previous briefing\n.\n&#xa0;\nA healthy market for parental \ncontrols is developing; everything proposed regarding filtering technology is available to parents already. Mandating network level filtering would amount to an intervention that could disrupt an emerging market for Internet access tools, whilst imposing s\nignificant costs on Internet Service Providers. \n&#xa0;\nThe Government's role should be to support this variety of tools and services by working with industry to ensure these are easily available and that parents understand how to use them.\n&#xa0;\nFiltering services b\nlock more than pornography\n&#xa0;\nSo far the debate has tended to focus on how filters should block 'pornography' or even more specifically 'hard core pornography'. Yet most if not all filtering systems are set up to block a range of categories of content extend\ning beyond adult sexual content. We urge the Committee to look at the variety of content that filtering systems block, how those categories are developed, who considers them to be '18 rated', and what sort of sites those offering filtering think should fal\nl under those categories. \n&#xa0;\nDifferent categories of material require different approaches\n&#xa0;\nToo often in the recent debate, a variety of different categories of material have been confused and muddled together. That has led to a confused discussion about th\ne appropriate way to deal with such a variety of material.\n&#xa0;\nFor example, child abuse images are illegal and dealt with by the Internet Watch Foundation. Other types of unlawful content may be subject to powers that can lead to content being taken down. \n&#xa0;\nO\nther material that is considered merely unsuitable for people of different ages however – and thus relevant when talking about home Internet filters - will involve more subjective judgments. Those will of course likely vary across households or value syste\nms. \nWhen talking about legal material, there is no easy way to define a category of unsuitable content that young people or children should be protected from. This is especially problematic when considering that filtering can happen in a variety of setti\nngs (libraries or shops and cafes or schools or homes or on devices taken across all of them), for a variety of age groups, and for people with a variety of values and beliefs.\n&#xa0;\nWe urge the Committee to keep a clear head about the very different types of m\naterial at issue and how each requires a different approach when considering how to create safer experiences for young people online across contexts. \n&#xa0;\nEnsuring that policy makers are asking the right questions\n&#xa0;\nWe have not been convinced that in pushing t\nhe filtering policies forward, policy makers this year have examined the most important practical questions closely enough. \n&#xa0;\nAs a result, we have asked 20 questions of ISPs (also noted in the introduction), which cover some of the most significant practic\nal issues with implementation of filtering services including privacy, how choices will be framed and how mistakes will be dealt with. We will submit answers when we receive them. We urge the Committee to look at these questions and consider the extent to \nwhich ISPs have addressed them adequately\n. \n&#xa0;\nImprove sexual health and relationships education\n&#xa0;\nWe support the work of academics such as Professor Andy \nPhippen\n, who in his research engages with young people in schools \nto talk about the pressures and problems they face in a world in which 'digital' and 'real' life are not distinct. \n&#xa0;\nWe support the suggestion that these issues are social problems facilitated by technology that cannot be fixed through technological fixes. \nWe also support the recommendation that we should endeavour to create an environment in which young people feel safe\n and supported in talking about \nthe problems they face. One aspect of this is to look at improving sexual health and relationship education. So we urge the Committee to look carefully at this context in which children and young people experience problemati\nc material online, rather than just at the limited tools available to try and limit access to some of it. \n&#xa0;\n&#xa0;\n2. Filtering out extremist material, including images of child abuse and material intended to promote terrorism or other acts of violence&#xa0;\nWe would\n firstly echo our previous comments. For example, as noted above, it is important for policy makers to recognise that we are talking about a variety of different categories of material. We should not assume that it is easy to define what content to filter \nout for which people or age group, and we should keep illegal and merely objectionable material distinct. \n&#xa0;\nOur comments above about the subjective judgments involved in deciding what content to filter are particularly relevant when looking at a broader ra\nnge of content. Words like 'violent', 'extreme', 'upsetting' or 'offensive' will have different meanings to people of different ages, beliefs and will this will vary across different settings. \n&#xa0;\nIf policy makers do push for an overly simple approach of spe\ncifying broad categories of legal but objectionable content to be filtered in a particular places, it is important to remember that this is not \nstopping\n these subjective judgement being made. Instead, those judgments are being made by a combination of thos\ne in charge of the provision of Internet access in that context (for example, perhaps a building maintenance service), the Internet service providers they use and the filtering service that ISP uses. Each will have their own attitude towards the \nblockable\n \ncategories and what material should fall within them.  Where these systems are opaque, as they often are, it just becomes harder to understand what decisions about access have been made. \n&#xa0;\n3. Preventing abusive or threatening comments on social media.&#xa0;We\n believe that an important aspect of dealing with this is to create a support system for young people with clear routes for them to report problems and find help.\n&#xa0;\nOnce again, it is important to distinguish between different types of behaviour, such as \noffensiveness, abusiveness or threats, which will require different approaches.\n&#xa0;\nWe also urge the Committee to look carefully at the issue of anonymity and \nrelatedly\n \npseudonymity\n. It is too easy to assume that tackling anonymity online is a simple solution\n to abusiveness. \n&#xa0;\nIn fact, people are usually not truly 'anonymous' when they are online. People leave all sorts of information that can identify them.  It is sometimes possible to use this information to identify somebody with varying levels of confidenc\ne – even if the person posts messages as an 'anonymous' or 'pseudonymous' user. For example an ISP may try to 'match' an IP address with one of their subscribers. There are various legal powers that in some \ncircumstance,\n require Internet companies to discl\nose this data, and which permit the use of it in various contexts for the purposes of trying to identify a user. \n&#xa0;\nFurther, anonymity in fact serves many positive purposes in a variety of circumstances. \nFor further \ninformation, please see our short introdu\nction to anonymity online, available from our website\n. \n&#xa0;\nSeptember 2013\n&#xa0;\n              \nhttps://www.openrightsgroup.org/ourwork/reports/mobile-internet-censorship:-whats-happening-and-what-we-can-do-about-it\n \n              \nhttps://www.openrightsgroup.org/ourwork/reports/response-to-dfe-consultation-on-parental-controls\n              \nhttps://www.openrightsgroup.org/ourwork/letters/open-letter-to-the-prime-minister-regarding-parental-internet-controls\n \n              \nhttps://www.openrightsgroup.org/ourwork/reports/parental-controls-and-internet-filtering-fact-sheet\n              \nhttps://www.openrightsgroup.org/ourwork/letters/culture-sec-content-restrictions\n \n              \nhttps://www.openrightsgroup.org/blog/20\n13/isp-filtering-qs\n \n              \nhttps://www.gov.uk/government/speeches/the-internet-and-pornography-prime-minister-calls-for-action\n \n              \nhttp://www2.lse.ac.uk/media@lse/research/EUKidsOnline/EU%20Kids%20III/Reports/ParentalMediation.pdf\n \n              \nhttps://www.openrightsgroup.org/blog/2013/website-filtering-problems-are-a-load-of-cock\n \n              \nht\ntp://urlchecker.o2.co.uk/urlcheck.aspx\n \n              \nSee Tim Davies, \nSangeet\n \nBhullar\n, and Terri \nDowty\n, “Rethinking responses to children and young people’s online lives”, September 2011, \nhttp://www2.lse.ac.uk/media@lse/research/%20EUKidsOnline/Conference%202011/Davies.pdf\n \n              \nhttps://www.openrig\nhtsgroup.org/blog/2013/online-gift-shop-blocked-by-mobile-networks\n \n              \nhttp://www.openrightsgroup.org/assets/files/files/pdfs/Net%20Filtering%20Brie\nf.pdf\n \n              \nhttps://www.openrightsgroup.org/blog/2013/isp-filtering-qs\n              \nhttp://www.openrightsgroup.org/assets/files/pdfs/ORG%20anonymity%20introduct\nion.pdf\n \n"