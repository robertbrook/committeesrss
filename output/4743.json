"&#xa0;\nWritten evidence submitted by John Christy (IPC0055)&#xa0;\n&#xa0;\nI am John R. Christy, \nDistinguished \nProfessor of Atmospheric Science, Alabama’s State Climatologist and Director of the Earth System Science Center at The University of Alabama in Huntsville.  I h\nave served as a Lead Author, \nContributing Author\n and Reviewer\n of IPCC assessments,\n have been awarded NASA’s Medal for Exceptional Scientific Achievement, and in 2002 \nwas \nelected a Fellow of the American Meteorological Society.  \n&#xa0;\nIt is a privil\nege for me to offer my views on the relationship between climate and weather \nbased on my exper\nience as a climate scientist.  My research area might be best described as building datasets from scratch to advance our understanding of what the climate is doing and why. I have used traditional surface observations as well as measurements from balloons \nand satellites to document the climate story.  Many of my datasets are used to test hypotheses of climate variability and change.  \n&#xa0;\nExtreme Events&#xa0;\nAs the glob\nal temperature failed to warm over\n the past 15 years, it became popular to \ndraw\n attention to the\n occurrence of \nextreme weather \nevents\n as worrisome \nconsequences\n of \npostulated climate change due to \nincreasing concentrations of greenhouse gases\n.  For example, m\nany claims have been made that \nweather\n events of the past 50 years are \n“\nunprecedented\n”\n, theref\nore must be caused by human influences\n.  However, o\nne can only\n establish \nsuch events as \nstatistically \nunusual\n, a lower standard than \n“\nunprecedented,\n”\n if \na minimum of\n 30 or more\n such periods with consistent data\n are available\n.  This means\n we need 1500 to 2000 years\n of information\n \nwith which to compare our recent 50-years \nof history \nto determine whether any characteristic is \ntruly \nunusual\n.  \n&#xa0;\nFor a few parameters we have such data.  Severe drought leaves a clear \nmark\n on the \nlandscape so th\nat\n we know our nation experienced droughts in the 12\nth\n century, the so-called mega\n-\ndroughts, which were much worse than any we’ve seen in the past century.\n  Thus, droughts of the past 50 years are not unusual \nand obviously not \n“\nunprecedented\n”\n as shown next\n.\n&#xa0;\nCaliforniaAt right are photos from Lindstrom (1990) \nof divers examining \ntrees \nwhich \ngrew on dry ground around 900 years ago in what is now a Sierra Nevada alpine lake.  This indicates that a drastic but natural change to a much drier climate \noccurred a\nnd must\n have lasted for at least \n50 years\n for trees to have grown to these sizes\n on dry ground.\n&#xa0;Rocky MountainsA 500-year history of moisture in the upper Colorado River basin (\nbelow\n) indicates the past century was quite moist while major multi-decadal droughts occurred in all four prior centuries (Piechota et al. 2004.)  Indeed, the conclusion of Piechota et al. states that after examining the paleo-record, the present-day droughts “\ncould be worse.” These and other evidences point to the real probability that water supply in the West will \nsee declines simply as a matter of the natural variability of climate\n.  \n&#xa0;Great PlainsIn the Great Plains, the period from 3000 to 1500 \nyears ago\n saw a drier and warmer climate during which a significant parabolic sand dune ecosystem developed, especially in western Nebraska and NE Colorado (Muhs 1985).\n  In other words, \nparts of \nthe Great Plains resembled a desert.\n  Many of these areas experienced \ndune “reactivation” during Medieval times (900-1300 AD).  The\nn, the\n climate moistened and cooled \nbeginning around\n 1300 AD to support the short-grass prairie seen today, though “reactivation” is possible at any time (Schmeisser, 2009).  Indeed, Muhs and Hol\nliday (1995) found that dune reactivation can occur within decadal time scales from extended drought by examining the Great Plains environmen\nt of only the past 150 years. \n&#xa0;\nWith the massive use of ground water for irrigation, the High Plains Aquifer has d\neclined an average of 12.8 ft, with some areas in the Texas panhandle down over 150 ft.   The key point here is that the Plains is subject to \nnatural (and sobering) \nlong-term droughts that would very likely tax the current water management system (ground-w\nater withdrawals) while not replenishing the aquifer, producing a situation of reduced agricultural productivity, especially in its southern reaches.\n&#xa0;\nU.S. Daily High Temperature RecordsAre daily high temperature extremes becoming more frequent?  To answe\nr such a question, one must obviously consider datasets that span an appropriate length of time.  If one \ndoes\n the analysis \nwith\n stations \nof\n at least 80 years of data, and determines the number of daily temperature records by year that stand as of 31 Dec 20\n12, the answer to the question is \n“\nno.\n”\n  It is true that the number of records in 2012 was quite high, thanks to a very warm March and a hot Mid-Western summer.  However in comparison to the heat waves of the 1930s, the summer was not the “worst” for heat.\n  \n2012 finished in 8\nth\n place on the list, just below \n6\nth\n and 7\nth\n places\n by a few days.\n  Imagine what this diagram would show if we had 1000 years of climate data in which it \nwould be \ncertainly likely that many years experienced more record warmth than even\n the 1930s.\n&#xa0;\nRecent TornadoesThe image to the right from NOAA indicates we are in a very low\n \ntornado period in our count\nr\ny – in fact the current year (\nright, \nblack line) is the lowest year-to-date (\nNov\n.) value in the 60-year history.  This of course is \nnot a prediction that tornadoes will decline in the future nor that there will be few tornadoes the rest of this year.  It is simply a recognition that the number of tornadoes can vary significantly from year to year and there is no long term trend (\nhttp://www.spc.noaa.gov/wcm/adj.html\n)\n&#xa0;\nRecent Wildfires&#xa0;\nWildfires are a natural \nconsequence\n of the U.S. climate\n variability\n and a feature to which many components of the natural ecosystem have found ways \nfor\n advantage-\ntaking\n.  Nowadays however, our fire su\nppression activities that allow\n \ne\nxcessive buildup of fuel combined with the careless or premeditated human character of some folks, gives greater opportunity for wildfires to \nbe started\n and to destroy.  The \ncurrent year has included the \nhuge\n Rim Fire in the central Sierra Nevada of California, but\n,\n on the whole\n,\n the year is well below average\n as shown in the graphic to the \nabove\n (data from the National Interagency Fire Center \nhttp://www.nifc.gov/fireInfo/fireInfo_stats_totalFires.html\n)\n.\n  A related metric is total snowfall in the Sierra\n of California\n which has \nalso \nshown no trend since the Southern Pacific Railroad Company began measuri\nng snowfall in 1878 (Christy 2012).\nWest Antarctica Coastal Temperatures&#xa0;Temperatures over the vast expanse of the Antarctic continent have not shown significant warming in the past several decades.  Indeed satellite-based observations of the \natmospheric temperature above Antarctica show a slight decline since 1979.  However, m\neasurements along the coast of West Antarctic \nand its \nPeninsula have warmed in recent years.  Thomas et al. (2013) have reconstructed 308 years of temperature variations (1702-2009) through stable isotopes and confirmed the recent warming.  They found that\n,\n&#xa0;\n… this warming trend is not unique.  More dramatic isotopic warming (and cooling) trends occurred in the mid-nineteenth and eighteenth centuries suggesting that at present, the effect of anthropogenic climate drivers at this location has not exceeded \nthe natural range of climate variability in the context of the past ~300 years.\n&#xa0;\nHere we have another example that indicates we must have hundreds of years of\n climate records \nbefore \ntrying to assess \nwhether \nrecent changes\n are unusual\n.  In this case, the te\nmperatures of West Antarc\ntica have experienced similar and likely \ngreater changes\n than recently observed\n in merely the las\nt 300 years, a period before which humans could have affected the climate.\n&#xa0;What does Extreme Weather really tell us?&#xa0;The point about\n our \nlack of \nunderstanding \nof t\nhe causes of ext\nreme weather was summed up in\n an\n article in \nNature\n magazine with the title “Extreme Weather – Better models are needed before exceptional events can be reliably linked to global warming” (\nNature\n, 20 September \n2012, vol 489, pg 335-6.) The emphasis in the article agrees with my statement that our level of understanding about the climate system is so low that we cannot predict nor attribute unusual events to human emissions of greenhouse gases\n using models and/or\n limited data records\n.  The article discusses the problem that current climate models are not “fit to inform legal and societal decisions” without further “enormous research” because at present they are not ready for such tasks.\n&#xa0;\nThe article notes that ext\nreme events “have complex causes, involving anomalies in atmospheric circulation, levels of soil moisture and the like.”  The comments of one scientist at a recent workshop on the topic indicated “the coarse and mathematically far-from-perfect climate mode\nls used to generate attribution claims … are unjustifiably speculative, basically unverifiable and better not made at all.”  Not all participants felt this way, however \nNature\n reported that, “None of the industry and government experts at the workshop coul\nd think of any concrete example in which an attribution might inform business or political decision-making.”  In other words, industry and government would prefer an accurate forecast over the notion of attributing that forecast to a particular cause.  Unf\nortunately, the ability to make accurate\n long-range forecasts is not here yet\n.\n&#xa0;\nIn the examples above, we don’t see increases in extreme events (which is also true for tornadoes, hurricanes, floods, etc. - see my House testimony of 31 March 2011) but we mu\nst certainly be ready for more to come as part of nature’s variability. \n&#xa0;\nI am not using \nthe examples above\n to prove the weather in the US is becoming less extreme.  My point is that extreme events are poor metrics to use for detecting climate \nchange.  In\ndeed, because of their rarity (by definition) using extreme events to bolster a claim about any type of climate change (warming or cooling) runs the risk of setting up the classic “non-falsifiable hypothesis.”    For example, we were told by the IPCC that \n“milder winter temperatures will decrease heavy snowstorms” (TAR WG2, 15.2.4.1.2.4).  After the winters of 2009-10 and 2010-11, we are told the opposite by advocates of the IPCC position, “Climate Change Makes Major Snowstorms More Likely” (\nhttp://www.ucsusa.org/news/press_release/climate-change-makes-snowstorms-more-likely-0506.html\n).\n&#xa0;\nThe non-falsifiable hypotheses can be stated this way, \n“whatever happens is consistent with my hypothesis.”  In other words, there is no event that would “falsify” the hypothesis.  As such, these assertions cannot be considered science\n,\n or in anyway informative\n,\n since the hypothesis’ fundamental prediction is \n“anything \ncan\n happen.”  In the example above if winters become milder or they become snowier, the non-falsifiable hypothesis stands.  This is not science.\n&#xa0;\nThere are innumerable types of events that can be defined as extreme events – so for the enterprisin\ng individual (unencumbered by the scientific method), weather statistics can supply an unlimited, target-rich environment in which to discover a “useful” extreme event. Thus, when the enterprising individual observes an unusual weather event, it may be tem\npting to define it as a once-for-all extreme metric to “prove” a point about climate change – even if the event was measured at a station with only 30 years of record. Extreme events happen, and their causes are intricately tied to the semi-unstable dynami\ncal situations that can occur out of an environment of natural, unforced variability.\n  In other words, Mother Nature ha\ns within her all the necessary tools\n to generate extreme events that exceed what we’ve seen in the past 50 years.\n&#xa0;\nScience checks hypothe\nses (assertions) by testing specific, falsifiable predictions implied by those hypotheses.  The predictions are to be made in a manner that, as much as possible, is blind to the data against which they are evaluated.  It is the testable predictions from\n a \nspecific set of\n hypotheses, \notherwise known as\n \nclimate model\n simulation\ns\n, that run into trouble as shown \nbelow\n.  Before going on to that test, the main point here is that extreme events do not lend themselves as being rigorous metrics for \nconvicting\n human \nCO2 emissions of being guilty of causing them.\n&#xa0;\nUtility of Climate Models&#xa0;\nIn the f\nigure\n below\n \nI provide the 35-\nyear record \n(1979-2013) \nof \natmospheric temperature in the tropics – the key region in which climate models respond to greenhouse gas warming wit\nh a large and distinct signal. \n The focus on the tropics is important because \nof the consistent and significant warming that climate models indicate should have already occurred as a result of the increasing concentration of greenhouse gases w\ne have put i\nnto the atmosphere. \n  \nIt also represents a part of the global atmosphere in which the critical water vapor and cloud feedbacks have major influences.\n  In addition, changes in this region were determined by the EPA to be a \nkey line of evidence of\n greenhouse-gas caused climate change.  Finally, t\nhe tropical atmosphere is also a huge and easy target for modeling projects to hit if the physics are well represented.  \nSince this warming should have taken place already, this provides for us a way to tes\nt the model simulations.\n There are 102 model runs represented in the figure, but I have organized them by the \n24 types of models\n.  The thick red line is the average of the 24 groups.  Thin, solid lines are the six model groupings created by U.S. institutio\nns and the dotted \nlines \nby those \nfrom \noutside the U.S.  The observations are provided by six independent \nsources\n, \nwith “balloons” being the average of the \nfour balloon-borne \ndatasets\n and \n“satellites” the average of the two \ngroups which utilize satellite in\nstrumentation.\n&#xa0;\nThe comparison\n shows \nt\nhat the very latest climate model simulations \nused in the IPCC Assessment released two \nmonths\n ago \nindicate that their response to CO2 on average is 2 to \n5\n times greater than reality. \n  In strict statistical testing, we can say that the m\nodels on average failed a simple\n hypothesis test \nto check whether they could represent\n the path the real world took on tropical \natmospheric \ntemperatures\n (see Douglass et\n al. 2007, McKitrick et al. 2010,\n 2011\n, Douglass and Christy 2013)\n.\n&#xa0;An \nextremely important\n paper was published in \nNature Climate Change\n this past spring as one of the first \nstudies \nto actually perform a test of model capabilities \nin a controlled experiment to\n understand the impacts on the critical processes that affect the way \nthe temperature will change\n (Stephens and Bony, 2013)\n.  They simply ran \nfour major climate models\n \nover\n an ocean-covered earth (i.e. a very simple earth) with the current ocean temperatur\nes, then again with elevated ocean temperatures.  The experiment would then reveal the impact of the extra warmth on the way the climate system operates\n, especially, clouds and rain because they have significant impacts on the warming processes. So, gettin\ng \nclouds and rain\n correct is necessary for long-term integrations. \nTo their surprise, the four major models gave quite different results\n (figure above)\n, both in terms of\n the\n magnitude \nand of the sign\n of the change in clouds \nand\n rain as shown in the figure.\n \nThis is exactly the type of fundamental, rigorous evaluation that must be encouraged for other parts of the modeling enterprise.  One can only conclude that at least three of the four models fail (if on the\n odd\n ch\nance o\nne is correct) to depict\n the\n fundame\nntal \nprocess\nes\n of the Earth system.  This result supports the comments in the paragraphs above which demonstrate the climate modeling enterprise must go “\nback to the\n basics” as stated in Stephens and Bony.\n&#xa0;\nIn a paper published last week\n, \nSwanson (2013) ex\namined the previous generation of climate models used in the IPCC AR4 (\n2007, \nknown as CMIP3) in comparison with the latest generation of models employed in the current IPCC AR5 (known as CMIP5 models as I used earlier).  Swanson found that the newer CMIP5 \nmodels were worse at depicting actual climate \nvariations \nthan the older CMIP3 versions.  He suggests that the modelers have a “selection bias based on warming rate” that attempts to replicate the rapid warming of the Arctic (a small region) while becoming \nworse (too warm) in the much more vast tropics and southern hemisphere.\n  He argues for a “healthy dose\n of diversity” to be reintroduced into the climate simulation enterprise.\n&#xa0;\nBasing scientific conclusions about climate change \n(\nor \nbasing \npolicy decisions \nabout energy\n)\n on \nclimate \nmodel output is risky given the \ninability of\n model simulations \nto reproduce the real\n world – and their results are not getting better.\n&#xa0;The IPCC Summary for Policy Makers&#xa0;\nRegarding the IPCC, please note that the IPCC was written by IPCC-selected scientists and that the document represents their opinions.  Many of the conclusions are fine but some of the key ones do not represent the views of \nmany in \nthe broader climate comm\nunity.  \n&#xa0;\nThe head\nline statement from the\n 2013\n \nSummary for Policy Makers\n baffles me.  It reads,\n&#xa0;\nIt is extremely likely that human influence has been the dominant cause of the observed warming since the mid-20\nth\n century.\n&#xa0;\nFirst, the IPCC \nrelies on\n climate \nmodels to distinguish “natural” from “human” caused climate change because instruments can’t.  However, as demonstrated, these same models on average fail \nby a significant amount \nto reproduce the climate of the past 35 years\n (the years most directly impact\ned by rising greenhouse gas emissions\n.\n)\n  But in conclusion, the IPCC now has\n even\n more confidence that the models can distinguish “natural” from \n“human” change\n \nover a period the models clearly fail to simulate well\n.  It doesn’t make sense to me.  \n&#xa0;\nNow, i\nt\n is true that \nin the models\n, most of the warming in the past 50 years is due to greenhouse gases, but since the model-based warming did not occur in reality (by a significant amount), how can one claim that reality was driven by greenhouse gas warming?\n&#xa0;\nI \nsee two things here, (1) \nthe\n need to go back to the drawing board on climate modeling \nwith special attention to the causes of natural variations and \nwith a rigorously independent \nvalidation\n program\n (i.e. a\n set of relatively inexpensive but\n true “\nRed Teams”)\n, and (2) the world community needs to be exposed to the real debates in climate science rather than statements amounting to a consensus of those who already agree with a\n certain\n consensus.\n  These are sentiments I have been advocating for years\n in congre\nssional testimony\n and\n which\n appear in an article published in \nNature\n magazine (Christy, \n2010\n see after references\n).\n  \n&#xa0;\nIn addition, \nI direct \nthe reader\n to \na supplement attached\n to \nthis\n written testimony by Professor Judith Curry of Georgia Tech entitled, “IPCC Diagnosis – Permanent Paradigm Paralysis.”\n  The title is an apt description of where the IPCC process has gone.\n&#xa0;\nSeventeen Years Ago – House Committee on Science&#xa0;\nSeventeen years \nago, in March 1996, I testified before this committee \nregarding\n climate change. In that testimony I reported on the development of the deep layer \natmospheric \ntemperature datasets \nfrom satellites \nthat Roy Spencer, then of NASA\n now of UAHuntsville\n, and I had\n pioneered.  Using these data\n,\n Richard McNider, also of UAHuntsville, and I wrote a paper in \nNature\n magazine that indicated climate model simulations were warming the planet about 4 times faster than in reality (Christy and McNider 1994).\n&#xa0;\n Further analysis\n confirmed a\n rate in models\n 2 to 4 times \nfaster than the\n real world\n.  \n&#xa0;\nIt was clear at the time, and agreed to by \nnearly everyone\n, that our understanding\n of how the climate system worked\n was poor and much more research was needed on observing the climate and on understanding its natural variations.\n&#xa0;\n I also noted that we should expect weather extremes \nto continue \nbecause that has been the nature of climate from the beginning. \nOne of my c\noncludin\ng statements was, and I quote, \n&#xa0;\nWithout a continuing program of research that places climate variations in proper perspective\n \n[i.e. \nnatural climate variations\n]\n and reports with improving confidence on their causes, we will be vulnerable to calls f\nor knee-jerk remedies to combat \"climate change,\" which likely will be unproduc\ntive and economically damaging.\n \n&#xa0;\nNow here we are, over 17 years later.  It appears the nation has indeed enacted “knee-jerk” remedies to \n“combat climate change”\n through regulat\nions on carbon dioxide.  I warned\n this committee\n in 1996 that these would be “unproductive and economically damaging.” I have since provided testimony that demonstrates that these regulations will be “unproductive” regarding their impact on climate.  I wil\nl leave it to economists to determine whether the regulations\n which result in higher energy prices\n are also “economically damaging”, especially for the poorest among us.\n&#xa0;\nThe nation did \nindeed \nsupport\n some\n efforts to \nimprove the\n \nclimate \nobserv\ning\n system, e\nspecially from space, \nto help in determining\n \nwhat \nwas happening with the climate, and then begin to understand \nwhy\n changes are taking place.  Other efforts seem to be falling by the wayside\n,\n including attention to the network of high quality surface monito\nring stations\n such as NOAA’s Regional Climate Reference Network\n.  Simply put, we need to know \nwhat\n the climate \ni\ns doing before claiming to know \nwhy\n it \nis doing what it i\ns doing\n.  W\nithout \naccurate \nobservations we \ncan\n not know \nwhat\n the climate is doing\n.\n&#xa0;\nIt is \nenlightening\n to examine the\n 35-\nyear comparison of models and observations of atmospheric temperature in the tropics – the key region in which climate models respond to greenhouse gas warming with a large and distinct signal\n and a region promoted by t\nhe EPA as a fingerprint of human-induced climate change\n.\n  This is an exceptionally large target for climate models to aim at, and it incorporates the critical water vapor and cloud feedbacks about which we know so little.   The current\n record is now twice \nas long as was available when I testified in 1996 and the models are more complicate\nd, expensive and numerous\n,\n \nrepresenting\n an industry unto itself\n.  The comparison\n \nshows that the very latest climate models’\n \ntropical \nresponse to CO2\n,\n on average\n,\n is still 2\n to \n5\n times greater than reality\n,\n \njust \nas \nit was\n in 1996. \n&#xa0;\nI believe we missed a tremendous opportunity \n17 years ago\n to develop a better understanding \nof the climate system \nbecause\n research dollars\n were directed\n to\n \nestablish\n a climate\n modeling industry\n.\n  \nTo \ncompound the problem\n as it developed\n, I believe\n we\n failed to\n fund substantial projects to\n examine the output of climate models in an independent, objective and method\nological\n \nway, i.e. \nwe did not establish “red teams”\n to rigorously study the output of m\nodels on which the most expensive of regulations \nnow \nrely.\n \n \nThis \nhas left us \n17 years later \nstill wondering\n \nwhat porti\non of th\ne \nrecent modest \nchange\n \nis\n natural and what portion might be\n human-caused.\n&#xa0;\nConclusion&#xa0;\nIn this testimony,\n evidence is presented to demonstrate that recent weather events are not outside the extremes that have occurred in the past \nwhen human influences were negligible\n.  Therefore\n in my view\n one cannot\n \nattribute \nthese recent events\n \nwith any confidence \nto\n someth\ning \nbeyond nature.  Climate models are \npromoted as tools that are able\n to discriminate \nnatural climate events\n versus \nthose that\n might happen as a result of the increases in greenhouse gases due to human activities\n and have been used by EPA for regulatory a\nction\n.  Unfortunately, as demonstrated here\n and discussed in the literature\n, climate models have not demonstrated acceptable skill in terms of depicting\n even\n \nvery \nfundamental, large-scale climate variations, and thus are unable to identify natural versus human-influenced events on regional scales.  Indeed, the lack of modeling skill regarding very basic processes such as tropical tropospheric variations, indica\ntes that the modeling enterprise has not been subject to rigorous, independent “Red Team” oversight during its expensive growth \nperiod.  In addition,\n significant advancements are needed in observing and understanding the natural processes of climate before\n reliable, though basic, forecasts are forthcoming.   \nIt is unfortunate\n, in my opinion,\n that recent policy has been made based on the projections of these faulty models.  \nClimate science has a long way to go.\n&#xa0;\nReferences:\n&#xa0;\nChristy, J.R. and R.T. McNider, \n1\n994: \nSatellite greenhouse signal.  \nNature\n, 376, p. 325.  Relevant quote, “Curve “e” reveals an upward trend of +0.09 °C per decade, or about one-quarter of the magnitude of climate model results.”\n&#xa0;\nChristy, J.R. 2010, “Open Debate, Wikipedia Style”, \nNature\n, vol 463, p 732.\n  (See below).\n&#xa0;\nDouglass, D.H., J.R. Christy, B.D. Pearson and S.F. Singer, 2007:  A comparison of tropical temperature trends with model predictions.  International J. Climatology, DOI: 10.1002/joc.1651.\n&#xa0;\nDouglass, D.H. and J.R. Christy, \n2013:  Reconciling observations of global temperature c\nhange: 2013.  Eng. and Environ.\n 24, 415-419.\n&#xa0;\nMcKitrick\n, R.R., S. McIntyre and C. Herman, (2010): Panel and multivariate  methods for tests of trend equivalence in climate data sets.\n&#xa0;\n Atmos. Sci. Lett.\n, 11(4), 270-277.\n&#xa0;\n doi: 10.1002/asl.290. \n&#xa0;\nMcKitrick, Ross, Stephen McIntyre and Chad Herman (2011)\n&#xa0;\nCorrection to \"Panel and Multivariate Methods for Tests of Trend Equivalence in \nClimate Data Series\n\"\n&#xa0;\nAtmospheric Science Letters\n&#xa0;\nOctober 7 2011,\n&#xa0;\nDOI: 10.1002/asl.360.\n&#xa0;\n&#xa0;\nSwanson,K. 2013: Emerging selection bias in large-scale climate change simulations.  Geophys. Res. 40, 3184-3188.  DOI:10.1002/grl50562.\n&#xa0;\nThomas, E.R., T.J. Bracegi\nrdle, J. Turner and E.W. Wolff, 2013: A 308 year record of climate variability in West Antarctica.  Geophys. Res. Lett., 40, 5492-5496.  DOI:10.1002/2013GL057782.\n&#xa0;\nDecember 2013\nPosted on\n \nSeptember 28, 2013\n \n|\n \n577 Comments\n \nby Judith Curry\nDiagnosis: paradigm paralysis, caused by motivated reasoning, oversimplification, and consensus seeking; worsened and made permane\nnt by a vicious positive feedback effect at the climate science-policy interface.\nIn a previous \npost\n, I discussed the IPCC’s diagnosis of a planetary \nfever and their prescription for planet Earth. \n&#xa0;\nIn this post, I provide a diagnosis and prescription for the IPCC.\nIn the 1990’s, the world’s nations embarked on a path to prevent dangerous anthropogenic climate change by stabilization of the concentration\ns of atmospheric greenhouse gases, which was codified by the 1992 UN Framework Convention on Climate Change (UNFCCC) treaty. The IPCC scientific assessments play a primary role in legitimizing national and international policies aimed at reducing greenhous\ne gas emissions. This objective has led to the IPCC assessments being framed around identifying anthropogenic influences on climate, dangerous environmental and socio-economic impacts of climate change, and stabilization of CO\n2\n concentrations in the atmosp\nhere.\nAt the time of establishment of the UNFCCC, there was as yet no clear signal of anthropogenic warming in the observations, as per the IPCC First Assessment Report (FAR) in 1990. It wasn’t until the IPCC’s Second Assessment Report in 1995 that a \n‘disc\nernible’\n human influence on global climate was identified. The scientific support for the UNFCCC treaty was not based on observations, but rather on our theoretical understanding of the greenhouse effect and simulations from global climate models.\n&#xa0;\n In the \nearly 1990’s there was the belief in the feasibility of reducing uncertainties in climate science and climate models, and a consensus seeking approach was formalized by the IPCC. General circulation climate models became elevated to the central role by pol\nicy actors and scientists from other fields investigating climate change impacts and applications – this has in turn has elevated the role and position of these climate models in climate change research. Very substantial investments have been made in furth\ner developing climate models, with the expectations that these models will provide actionable information for policy makers.\nIn 2006/2007, climate change had soared to the top of the international political agenda, as a result of Hurricane Katrina, Al Gore\n’s \nAn Inconvenient Truth\n, publication of the IPCC AR4 in 2007, and award of the Nobel Peace Prize to Al Gore and the IPCC.\n&#xa0;\n It was claimed that the science was settled, and that it clearly demanded radical policy and governmental action to substantially cu\nt CO2 emissions.\nSymptoms of the diseaseSeven years later, with the release of the IPCC AR5, we find ourselves between the metaphorical rock and a hard place with regards to climate science and policy:\nAnd finally:\nDiagnosis of the cause of the diseaseHow and why did we land between a rock and a hard place on the climate change issu\ne?\n&#xa0;\n There are probably many contributing reasons, but the most fundamental and profound reason is arguably that both the problem and solution were vastly oversimplified back in 1990 by the UNFCCC/IPCC, where the framed both the problem and the solution as \nirreducibly global. This framing was locked in by a self-reinforcing consensus-seeking approach to the science and a ‘speaking consensus to power’ approach for decision making that pointed to only one possible course of policy action – radical emissions re\nductions. The climate community has worked for more than 20 years to establish a scientific consensus on anthropogenic climate change. The IPCC consensus building process played a useful role in the early synthesis of the scientific knowledge. However, the\n ongoing scientific consensus seeking process has had the unintended consequence of oversimplifying both the problem and its solution and hyper-politicizing both, introducing biases into the both the science and related decision making processes.\nIn their \nWrong Trousers\n essay, Prins and Rayner\n&#xa0;\nargue that we have made the wrong cognitive choices in our attempts to define the problem of climate change,\n by relying on strategies that worked previously with ozone, sulphur emissions and nuclear bombs. While these issues may share some superficial similarities with the climate change problems, they are ‘tame’ problems (complicated, but with defined and achie\nvable end-states), whereas climate change is ‘wicked’ (comprising open, complex and imperfectly understood systems). For wicked problems, effective policy requires profound integration of technical knowledge with understanding of social and natural systems\n. In a wicked problem, there is no end to causal chains in interacting open systems, and every wicked problem can be considered as a symptom of another problem; if we attempt to simplify the problem, we become risk becoming prisoners of our own assumptions\n.\nThe framing of the climate change problem by the UNFCCC/IPCC and the early articulation of a preferred policy option by the UNFCCC has arguably marginalized research on broader issues surrounding climate variability and change, resulting in an overconfid\nent assessment of the importance of greenhouse gases in future climate change and stifling the development of a broader range of policy options.\n&#xa0;\n The result of this simplified framing of a wicked problem is that we lack the kinds of information to more bro\nadly understand climate change and societal vulnerability.\nParadigm paralysis is the inability or refusal to see beyond the current models of thinking. The vast amount of scientific and political capital invested in the IPCC has become self-reinforcing, so\n it is not clear how move past this paralysis as long as the IPCC remains in existence. The wickedness of the climate change problem makes if difficult to identify points of irrefutable failure in either the science or the policies, although the IPCC’s ins\nistence that the pause is irrelevant and temporary could provide just such a refutation if the pause continues. In any event, there is a growing realization of that neither the science or policy efforts are making much progress, and particularly in view of\n the failure climate models to predict the stagnation in warming, and that perhaps it is time to step back and see if we can do a better job of understanding and predicting climate variability and change and reducing societal and ecosystem vulnerabilities.\nBroader implications of the disease&#xa0;Specifically with regards to climate research, for the past decade most of the resources have been expended on providing projections of future climate change using complex Earth system models, assessing and interpretin\ng the output of climate models, and application of the output of climate models by the climate impacts community.\nThe large investment in climate modeling, both in the U.S. and internationally, has been made with the expectation that climate models will su\npport decision making on both mitigation and adaptation responses to climate change. \n&#xa0;\nSo, are these complex global climate models especially useful for decision makers?\n&#xa0;\n The hope, and the potential, of climate models for providing credible regional climate\n change scenarios have not been realized.\n&#xa0;\n \nWith the failure of climate models to simulate the pause and regional climate variability, we have arguably reached the point of diminishing returns from this particular path of climate modeling – not just for de\ncision support but also for scientific understanding of the climate system.\n&#xa0;\n In pursuit of this climate modeling path, the climate modeling community — and the funding agencies and the policy makers — have locked themselves into a single climate modeling f\nramework with a focus on production runs for the IPCC, which has been very expensive in terms of funding and personnel. An unintended consequence of this strategy is that there has been very little left over for true climate modeling innovations and fundam\nental research into climate dynamics and theory — such research would not only support amelioration of deficiencies and failures in the current climate modeling systems, but would also lay the foundations for disruptive advances in our understanding of the\n climate system and our ability to predict emergent phenomena such as abrupt climate change.\nAs a result, we’ve lost a generation of climate dynamicists, who have been focused on climate models rather than on\n \nclimate dynamics and theory that is needed to \nunderstand the effects of the sun on climate, the network of natural internal variability on multiple time scales, the mathematics of extreme events, and predictability of a complex system characterized by spatio-temporal chaos. New structural forms are ne\neded for climate models that are capable of simulating the natural internal variability of the coupled ocean-atmosphere system on timescales from days to millennia and that can accurately account for the fast thermodynamic feedback processes associated wit\nh clouds and water vapor.\nHoping and expecting to rely on information from climate models about projected regional climate change to guide adaptation response has diverted attention from using observational, historical and paleoclimate data from the region\n to more usefully develop the basis for future scenarios. Further, increased scientific focus on subseasonal (weeks) and seasonal (months) weather/climate forecasts could produce the basis for tactical adaptation practices with substantial societal benefit\ns.\nSecuring the common interest on local and regional scales (referred to by Brunner and Lynch as “\nadaptive governance\n”) provides the rationale for effective\n climate adaptation strategies. This requires abandoning the irreducibly global consensus seeking approach in favor of open debate and discussion of a broad range of policy options that stimulate local and regional solutions to the multifaceted and interre\nlated issues surrounding climate change.\nThe IPCC needs to get out of the way so that scientists and policy makers can better do their jobs.\nConclusionThe diagnosis of paradigm paralysis seems fatal in the case of the IPCC, given the widespread nature of \nthe infection and intrinsic motivated reasoning.\n&#xa0;\n We need to put down the IPCC as soon as possible – not to protect the patient who seems to be thriving in its own little cocoon, but for the sake of the rest of us whom it is trying to infect with its disea\nse.\n&#xa0;\n Fortunately much of the population seems to be immune, but some governments seem highly susceptible to the disease. However, the precautionary principle demands that we not take any risks here, and hence the IPCC should be put down.\n&#xa0;\n&#xa0;\n&#xa0;\n"